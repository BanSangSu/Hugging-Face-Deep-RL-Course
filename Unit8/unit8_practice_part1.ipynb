{"cells":[{"cell_type":"markdown","source":["**TODO and the second line codes are my exercise**"],"metadata":{"id":"1zXqarp-eG0M"}},{"cell_type":"markdown","metadata":{"id":"-cf5-oDPjwf8"},"source":["# Unit 8: Proximal Policy Gradient (PPO) with PyTorch ü§ñ\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail.png\" alt=\"Unit 8\"/>\n","\n","\n","In this notebook, you'll learn to **code your PPO agent from scratch with PyTorch using CleanRL implementation as model**.\n","\n","To test its robustness, we're going to train it in:\n","\n","- [LunarLander-v2 üöÄ](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n"]},{"cell_type":"markdown","metadata":{"id":"2Fl6Rxt0lc0O"},"source":["‚¨áÔ∏è Here is an example of what you will achieve. ‚¨áÔ∏è"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"DbKfCj5ilgqT","colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"status":"ok","timestamp":1694792090208,"user_tz":-540,"elapsed":15,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}},"outputId":"b9ba1114-b408-4659-ac3e-4a24e577bb59"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n"]},"metadata":{}}],"source":["%%html\n","<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"]},{"cell_type":"markdown","metadata":{"id":"YcOFdWpnlxNf"},"source":["We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."]},{"cell_type":"markdown","source":["## Objectives of this notebook üèÜ\n","\n","At the end of the notebook, you will:\n","\n","- Be able to **code your PPO agent from scratch using PyTorch**.\n","- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n","\n","\n"],"metadata":{"id":"T6lIPYFghhYL"}},{"cell_type":"markdown","source":["## This notebook is from the Deep Reinforcement Learning Course\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>\n","\n","In this free course, you will:\n","\n","- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n","- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n","- ü§ñ Train **agents in unique environments**\n","\n","Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n","\n","\n","The best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5"],"metadata":{"id":"Wp-rD6Fuhq31"}},{"cell_type":"markdown","source":["## Prerequisites üèóÔ∏è\n","Before diving into the notebook, you need to:\n","\n","üî≤ üìö Study [PPO by reading Unit 8](https://huggingface.co/deep-rl-course/unit8/introduction) ü§ó  "],"metadata":{"id":"rasqqGQlhujA"}},{"cell_type":"markdown","source":["To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push one model, we don't ask for a minimal result but we **advise you to try different hyperparameters settings to get better results**.\n","\n","If you don't find your model, **go to the bottom of the page and click on the refresh button**\n","\n","For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"],"metadata":{"id":"PUFfMGOih3CW"}},{"cell_type":"markdown","source":["## Set the GPU üí™\n","- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"],"metadata":{"id":"PU4FVzaoM6fC"}},{"cell_type":"markdown","source":["- `Hardware Accelerator > GPU`\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"],"metadata":{"id":"KV0NyFdQM9ZG"}},{"cell_type":"markdown","source":["## Create a virtual display üîΩ\n","\n","During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n","\n","Hence the following cell will install the librairies and create and run a virtual screen üñ•"],"metadata":{"id":"bTpYcVZVMzUI"}},{"cell_type":"code","source":["!pip install setuptools==65.5.0"],"metadata":{"id":"Fd731S8-NuJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install setuptools==65.5.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FWTDxvofNxA","executionInfo":{"status":"ok","timestamp":1695096870691,"user_tz":-540,"elapsed":7969,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}},"outputId":"644dcd71-6980-4924-a916-a86ad3ee625c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: setuptools==65.5.0 in /usr/local/lib/python3.10/dist-packages (65.5.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jV6wjQ7Be7p5"},"outputs":[],"source":["%%capture\n","!apt install python-opengl\n","!apt install ffmpeg\n","!apt install xvfb\n","!apt install swig cmake\n","!pip install pyglet==1.5\n","!pip3 install pyvirtualdisplay"]},{"cell_type":"code","source":["%%capture\n","!apt install python-opengl\n","!apt install ffmpeg\n","!apt install xvfb\n","!apt install swig cmake\n","!pip install pyglet==1.5\n","!pip3 install pyvirtualdisplay"],"metadata":{"id":"X7fAyy53flpG","executionInfo":{"status":"ok","timestamp":1695096891849,"user_tz":-540,"elapsed":21163,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"],"metadata":{"id":"ww5PQH1gNLI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AspBX1BMfmJ1","executionInfo":{"status":"ok","timestamp":1695096891850,"user_tz":-540,"elapsed":12,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}},"outputId":"49514ac9-8af9-494c-a55a-6023fa805c27"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f6d4e7f7760>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"ncIgfNf3mOtc"},"source":["## Install dependencies üîΩ\n","For this exercise, we use `gym==0.22`."]},{"cell_type":"code","source":["!pip install gym==0.22\n","!pip install imageio-ffmpeg\n","!pip install huggingface_hub\n","!pip install gym[box2d]==0.22"],"metadata":{"id":"9xZQFTPcsKUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gym==0.22\n","!pip install imageio-ffmpeg\n","!pip install huggingface_hub\n","!pip install gym[box2d]==0.22"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ch1Sz5CWfzeR","executionInfo":{"status":"ok","timestamp":1695096916454,"user_tz":-540,"elapsed":24613,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}},"outputId":"9359f847-f078-4349-9949-19aea4b6ffa2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym==0.22 in /usr/local/lib/python3.10/dist-packages (0.22.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.22) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.22) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.22) (0.0.8)\n","Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.4.8)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.17.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n","Requirement already satisfied: gym[box2d]==0.22 in /usr/local/lib/python3.10/dist-packages (0.22.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.22) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.22) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.22) (0.0.8)\n","Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.22) (2.3.5)\n","Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.22) (2.1.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"oDkUufewmq6v"},"source":["## Let's code PPO from scratch with Costa Huang tutorial\n","- For the core implementation of PPO we're going to use the excellent [Costa Huang](https://costa.sh/) tutorial.\n","- In addition to the tutorial, to go deeper you can read the 37 core implementation details: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n","\n","üëâ The video tutorial: https://youtu.be/MEt6rrxH8W4"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"aNgEL1_uvhaq","colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"status":"ok","timestamp":1694792447503,"user_tz":-540,"elapsed":439,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}},"outputId":"48605f02-5318-4fc0-e752-d173ac03fe81"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n","  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"]},{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"]},"metadata":{},"execution_count":5}],"source":["from IPython.display import HTML\n","\n","HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"]},{"cell_type":"markdown","metadata":{"id":"f34ILn7AvTbt"},"source":["- The best is to code first on the cell below, this way, if you kill the machine **you don't loose the implementation**."]},{"cell_type":"markdown","source":["### My ppo.py code"],"metadata":{"id":"xWwRLDoxaIqA"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"_bE708C6mhE7","colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"status":"error","timestamp":1695092072657,"user_tz":-540,"elapsed":8920,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}},"outputId":"f9045608-d5e9-49aa-c287-a3fb661d58b3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n","  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d07d4e634c24>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupload_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepocard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetadata_eval_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'huggingface_hub'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["### My ppo.py code\n","import argparse\n","import os\n","import random\n","import time\n","from distutils.util import strtobool\n","\n","import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions.categorical import Categorical\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from huggingface_hub import HfApi, upload_folder\n","from huggingface_hub.repocard import metadata_eval_result, metadata_save\n","\n","from pathlib import Path\n","import datetime\n","import tempfile\n","import json\n","import shutil\n","import imageio\n","\n","from wasabi import Printer\n","msg = Printer()\n","\n","def parse_args():\n","    # fmt: off\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n","        help=\"the name of this experiment\")\n","    parser.add_argument(\"--seed\", type=int, default=1,\n","        help=\"seed of the experiment\")\n","    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n","    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"if toggled, cuda will be enabled by default\")\n","    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n","        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n","    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n","        help=\"the wandb's project name\")\n","    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n","        help=\"the entity (team) of wandb's project\")\n","    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n","        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n","\n","    # Algorithm specific arguments\n","    parser.add_argument(\"--env-id\", type=str, default=\"CartPole-v1\",\n","        help=\"the id of the environment\")\n","    parser.add_argument(\"--total-timesteps\", type=int, default=50000,\n","        help=\"total timesteps of the experiments\")\n","    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n","        help=\"the learning rate of the optimizer\")\n","    parser.add_argument(\"--num-envs\", type=int, default=4,\n","        help=\"the number of parallel game environments\")\n","    parser.add_argument(\"--num-steps\", type=int, default=128,\n","        help=\"the number of steps to run in each environment per policy rollout\")\n","    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"Toggle learning rate annealing for policy and value networks\")\n","    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"Use GAE for advantage computation\")\n","    parser.add_argument(\"--gamma\", type=float, default=0.99,\n","        help=\"the discount factor gamma\")\n","    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n","        help=\"the lambda for the general advantage estimation\")\n","    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n","        help=\"the number of mini-batches\")\n","    parser.add_argument(\"--update-epochs\", type=int, default=4,\n","        help=\"the K epochs to update the policy\")\n","    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"Toggles advantages normalization\")\n","    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n","        help=\"the surrogate clipping coefficient\")\n","    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n","    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n","        help=\"coefficient of the entropy\")\n","    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n","        help=\"coefficient of the value function\")\n","    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n","        help=\"the maximum norm for the gradient clipping\")\n","    parser.add_argument(\"--target-kl\", type=float, default=None,\n","        help=\"the target KL divergence threshold\")\n","\n","    # HuggingFace Argument\n","    parser.add_argument(\"--repo-id\", type=str, default=\"BanUrsus/my-ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n","\n","    args = parser.parse_args()\n","    args.batch_size = int(args.num_envs * args.num_steps)\n","    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n","    # fmt: on\n","    return args\n","\n","def package_to_hub(repo_id,\n","                model,\n","                hyperparameters,\n","                eval_env,\n","                video_fps=30,\n","                commit_message=\"Push agent to the Hub\",\n","                token= None,\n","                logs=None\n","                ):\n","  \"\"\"\n","  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n","  This method does the complete pipeline:\n","  - It evaluates the model\n","  - It generates the model card\n","  - It generates a replay video of the agent\n","  - It pushes everything to the hub\n","  :param repo_id: id of the model repository from the Hugging Face Hub\n","  :param model: trained model\n","  :param eval_env: environment used to evaluate the agent\n","  :param fps: number of fps for rendering the video\n","  :param commit_message: commit message\n","  :param logs: directory on local machine of tensorboard logs you'd like to upload\n","  \"\"\"\n","  msg.info(\n","        \"This function will save, evaluate, generate a video of your agent, \"\n","        \"create a model card and push everything to the hub. \"\n","        \"It might take up to 1min. \\n \"\n","        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n","    )\n","  # Step 1: Clone or create the repo\n","  repo_url = HfApi().create_repo(\n","        repo_id=repo_id,\n","        token=token,\n","        private=False,\n","        exist_ok=True,\n","    )\n","\n","  with tempfile.TemporaryDirectory() as tmpdirname:\n","    tmpdirname = Path(tmpdirname)\n","\n","    # Step 2: Save the model\n","    torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n","\n","    # Step 3: Evaluate the model and build JSON\n","    mean_reward, std_reward = _evaluate_agent(eval_env,\n","                                           10,\n","                                           model)\n","\n","    # First get datetime\n","    eval_datetime = datetime.datetime.now()\n","    eval_form_datetime = eval_datetime.isoformat()\n","\n","    evaluate_data = {\n","        \"env_id\": hyperparameters.env_id,\n","        \"mean_reward\": mean_reward,\n","        \"std_reward\": std_reward,\n","        \"n_evaluation_episodes\": 10,\n","        \"eval_datetime\": eval_form_datetime,\n","    }\n","\n","    # Write a JSON file\n","    with open(tmpdirname / \"results.json\", \"w\") as outfile:\n","      json.dump(evaluate_data, outfile)\n","\n","    # Step 4: Generate a video\n","    video_path =  tmpdirname / \"replay.mp4\"\n","    record_video(eval_env, model, video_path, video_fps)\n","\n","    # Step 5: Generate the model card\n","    generated_model_card, metadata = _generate_model_card(\"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters)\n","    _save_model_card(tmpdirname, generated_model_card, metadata)\n","\n","    # Step 6: Add logs if needed\n","    if logs:\n","      _add_logdir(tmpdirname, Path(logs))\n","\n","    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n","\n","    repo_url = upload_folder(\n","            repo_id=repo_id,\n","            folder_path=tmpdirname,\n","            path_in_repo=\"\",\n","            commit_message=commit_message,\n","            token=token,\n","        )\n","\n","    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n","  return repo_url\n","\n","def _evaluate_agent(env, n_eval_episodes, policy):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param policy: The agent\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in range(n_eval_episodes):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards_ep = 0\n","\n","    while done is False:\n","      state = torch.Tensor(state).to(device)\n","      action, _, _, _ = policy.get_action_and_value(state)\n","      new_state, reward, done, info = env.step(action.cpu().numpy())\n","      total_rewards_ep += reward\n","      if done:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward\n","\n","\n","def record_video(env, policy, out_directory, fps=30):\n","  images = []\n","  done = False\n","  state = env.reset()\n","  img = env.render(mode='rgb_array')\n","  images.append(img)\n","  while not done:\n","    state = torch.Tensor(state).to(device)\n","    # Take the action (index) that have the maximum expected future reward given that state\n","    action, _, _, _  = policy.get_action_and_value(state)\n","    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n","    img = env.render(mode='rgb_array')\n","    images.append(img)\n","  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n","\n","\n","def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n","  \"\"\"\n","  Generate the model card for the Hub\n","  :param model_name: name of the model\n","  :env_id: name of the environment\n","  :mean_reward: mean reward of the agent\n","  :std_reward: standard deviation of the mean reward of the agent\n","  :hyperparameters: training arguments\n","  \"\"\"\n","  # Step 1: Select the tags\n","  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n","\n","  # Transform the hyperparams namespace to string\n","  converted_dict = vars(hyperparameters)\n","  converted_str = str(converted_dict)\n","  converted_str = converted_str.split(\", \")\n","  converted_str = '\\n'.join(converted_str)\n","\n","  # Step 2: Generate the model card\n","  model_card = f\"\"\"\n","  # PPO Agent Playing {env_id}\n","\n","  This is a trained model of a PPO agent playing {env_id}.\n","\n","  # Hyperparameters\n","  ```python\n","  {converted_str}\n","  ```\n","  \"\"\"\n","  return model_card, metadata\n","\n","def generate_metadata(model_name, env_id, mean_reward, std_reward):\n","  \"\"\"\n","  Define the tags for the model card\n","  :param model_name: name of the model\n","  :param env_id: name of the environment\n","  :mean_reward: mean reward of the agent\n","  :std_reward: standard deviation of the mean reward of the agent\n","  \"\"\"\n","  metadata = {}\n","  metadata[\"tags\"] = [\n","        env_id,\n","        \"ppo\",\n","        \"deep-reinforcement-learning\",\n","        \"reinforcement-learning\",\n","        \"custom-implementation\",\n","        \"deep-rl-course\"\n","  ]\n","\n","  # Add metrics\n","  eval = metadata_eval_result(\n","      model_pretty_name=model_name,\n","      task_pretty_name=\"reinforcement-learning\",\n","      task_id=\"reinforcement-learning\",\n","      metrics_pretty_name=\"mean_reward\",\n","      metrics_id=\"mean_reward\",\n","      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n","      dataset_pretty_name=env_id,\n","      dataset_id=env_id,\n","  )\n","\n","  # Merges both dictionaries\n","  metadata = {**metadata, **eval}\n","\n","  return metadata\n","\n","def _save_model_card(local_path, generated_model_card, metadata):\n","    \"\"\"Saves a model card for the repository.\n","    :param local_path: repository directory\n","    :param generated_model_card: model card generated by _generate_model_card()\n","    :param metadata: metadata\n","    \"\"\"\n","    readme_path = local_path / \"README.md\"\n","    readme = \"\"\n","    if readme_path.exists():\n","        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n","            readme = f.read()\n","    else:\n","        readme = generated_model_card\n","\n","    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n","        f.write(readme)\n","\n","    # Save our metrics to Readme metadata\n","    metadata_save(readme_path, metadata)\n","\n","def _add_logdir(local_path: Path, logdir: Path):\n","  \"\"\"Adds a logdir to the repository.\n","  :param local_path: repository directory\n","  :param logdir: logdir directory\n","  \"\"\"\n","  if logdir.exists() and logdir.is_dir():\n","    # Add the logdir to the repository under new dir called logs\n","    repo_logdir = local_path / \"logs\"\n","\n","    # Delete current logs if they exist\n","    if repo_logdir.exists():\n","      shutil.rmtree(repo_logdir)\n","\n","    # Copy logdir into repo logdir\n","    shutil.copytree(logdir, repo_logdir)\n","\n","# -- model\n","\n","def make_env(env_id, seed, idx, capture_video, run_name):\n","    def thunk():\n","        env = gym.make(env_id)\n","        env = gym.wrappers.RecordEpisodeStatistics(env)\n","        if capture_video:\n","            if idx == 0:\n","                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n","        env.seed(seed)\n","        env.action_space.seed(seed)\n","        env.observation_space.seed(seed)\n","        return env\n","\n","    return thunk\n","\n","\n","def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n","    torch.nn.init.orthogonal_(layer.weight, std)\n","    torch.nn.init.constant_(layer.bias, bias_const)\n","    return layer\n","\n","\n","class Agent(nn.Module):\n","    def __init__(self, envs):\n","        super().__init__()\n","        self.critic = nn.Sequential(\n","            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 1), std=1.0),\n","        )\n","        self.actor = nn.Sequential(\n","            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n","        )\n","\n","    def get_value(self, x):\n","        return self.critic(x)\n","\n","    def get_action_and_value(self, x, action=None):\n","        logits = self.actor(x)\n","        probs = Categorical(logits=logits)\n","        if action is None:\n","            action = probs.sample()\n","        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n","\n","\n","if __name__ == \"__main__\":\n","    args = parse_args()\n","    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n","    if args.track:\n","        import wandb\n","\n","        wandb.init(\n","            project=args.wandb_project_name,\n","            entity=args.wandb_entity,\n","            sync_tensorboard=True,\n","            config=vars(args),\n","            name=run_name,\n","            monitor_gym=True,\n","            save_code=True,\n","        )\n","    writer = SummaryWriter(f\"runs/{run_name}\")\n","    writer.add_text(\n","        \"hyperparameters\",\n","        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n","    )\n","\n","    # TRY NOT TO MODIFY: seeding\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","\n","    # env setup\n","    envs = gym.vector.SyncVectorEnv(\n","        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n","    )\n","    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n","\n","    agent = Agent(envs).to(device)\n","    optimiser = optim.AdamW(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n","\n","    # ALGO Logic: Storage setup\n","    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n","    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n","    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","\n","    # TRY NOT TO MODIFY: start the game\n","    global_step = 0\n","    start_time = time.time()\n","    next_obs = torch.Tensor(envs.reset()).to(device)\n","    next_done = torch.zeros(args.num_envs).to(device)\n","    num_updates = args.total_timesteps // args.batch_size\n","\n","    for update in range(1, num_updates + 1):\n","        # Annealing the rate if instructed to do so.\n","        if args.anneal_lr:\n","            frac = 1.0 - (update - 1.0) / num_updates\n","            lrnow = frac * args.learning_rate\n","            optimiser.param_groups[0][\"lr\"] = lrnow\n","\n","        for step in range(0, args.num_steps):\n","            global_step += 1 * args.num_envs\n","            obs[step] = next_obs\n","            dones[step] = next_done\n","\n","            # ALGO LOGIC: action logic\n","            with torch.no_grad():\n","                action, logprob, _, value = agent.get_action_and_value(next_obs)\n","                values[step] = value.flatten()\n","            actions[step] = action\n","            logprobs[step] = logprob\n","\n","            # TRY NOT TO MODIFY: execute the game and log data.\n","            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n","            rewards[step] = torch.tensor(reward).to(device).view(-1)\n","            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n","\n","            for item in info:\n","                if \"episode\" in item.keys():\n","                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n","                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n","                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n","                    break\n","\n","        # bootstrap value if not done\n","        with torch.no_grad():\n","            next_value = agent.get_value(next_obs).reshape(1, -1)\n","            if args.gae:\n","                advantages = torch.zeros_like(rewards).to(device)\n","                lastgaelam = 0\n","                for t in reversed(range(args.num_steps)):\n","                    if t == args.num_steps - 1:\n","                        nextnonterminal = 1.0 - next_done\n","                        nextvalues = next_value\n","                    else:\n","                        nextnonterminal = 1.0 - dones[t + 1]\n","                        nextvalues = values[t + 1]\n","                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n","                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n","                returns = advantages + values\n","            else:\n","                returns = torch.zeros_like(rewards).to(device)\n","                for t in reversed(range(args.num_steps)):\n","                    if t == args.num_steps - 1:\n","                        nextnonterminal = 1.0 - next_done\n","                        next_return = next_value\n","                    else:\n","                        nextnonterminal = 1.0 - dones[t + 1]\n","                        next_return = returns[t + 1]\n","                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n","                advantages = returns - values\n","\n","        # flatten the batch\n","        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n","        b_logprobs = logprobs.reshape(-1)\n","        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n","        b_advantages = advantages.reshape(-1)\n","        b_returns = returns.reshape(-1)\n","        b_values = values.reshape(-1)\n","\n","        # Optimizing the policy and value network\n","        b_inds = np.arange(args.batch_size)\n","        clipfracs = []\n","        for epoch in range(args.update_epochs):\n","            np.random.shuffle(b_inds)\n","            for start in range(0, args.batch_size, args.minibatch_size):\n","                end = start + args.minibatch_size\n","                mb_inds = b_inds[start:end]\n","\n","                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n","                logratio = newlogprob - b_logprobs[mb_inds]\n","                ratio = logratio.exp()\n","\n","                with torch.no_grad():\n","                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n","                    old_approx_kl = (-logratio).mean()\n","                    approx_kl = ((ratio - 1) - logratio).mean()\n","                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n","\n","                mb_advantages = b_advantages[mb_inds]\n","                if args.norm_adv:\n","                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n","\n","                # Policy loss\n","                pg_loss1 = -mb_advantages * ratio\n","                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n","                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n","\n","                # Value loss\n","                newvalue = newvalue.view(-1)\n","                if args.clip_vloss:\n","                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n","                    v_clipped = b_values[mb_inds] + torch.clamp(\n","                        newvalue - b_values[mb_inds],\n","                        -args.clip_coef,\n","                        args.clip_coef,\n","                    )\n","                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n","                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n","                    v_loss = 0.5 * v_loss_max.mean()\n","                else:\n","                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n","\n","                entropy_loss = entropy.mean()\n","                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n","\n","                optimiser.zero_grad()\n","                loss.backward()\n","                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n","                optimiser.step()\n","\n","            if args.target_kl is not None:\n","                if approx_kl > args.target_kl:\n","                    break\n","\n","        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n","        var_y = np.var(y_true)\n","        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n","\n","        # TRY NOT TO MODIFY: record rewards for plotting purposes\n","        writer.add_scalar(\"charts/learning_rate\", optimiser.param_groups[0][\"lr\"], global_step)\n","        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n","        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n","        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n","        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n","        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n","        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n","        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n","        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n","        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n","\n","    envs.close()\n","    writer.close()\n","\n","    # Create the evaluation environment\n","    eval_env = gym.make(args.env_id)\n","\n","    package_to_hub(repo_id = args.repo_id,\n","                model = agent, # The model we want to save\n","                hyperparameters = args,\n","                eval_env = gym.make(args.env_id),\n","                logs = f\"runs/{run_name}\",\n","                )\n"]},{"cell_type":"markdown","metadata":{"id":"mk-a9CmNuS2W"},"source":["## Add the Hugging Face Integration ü§ó\n","- In order to push our model to the Hub, we need to define a function `package_to_hub`"]},{"cell_type":"markdown","metadata":{"id":"TPi1Nme-oGWd"},"source":["- Add dependencies we need to push our model to the Hub"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Sj8bz-AmoNVj","executionInfo":{"status":"ok","timestamp":1694871111139,"user_tz":-540,"elapsed":4,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}}},"outputs":[],"source":["from huggingface_hub import HfApi, upload_folder\n","from huggingface_hub.repocard import metadata_eval_result, metadata_save\n","\n","from pathlib import Path\n","import datetime\n","import tempfile\n","import json\n","import shutil\n","import imageio\n","\n","from wasabi import Printer\n","msg = Printer()"]},{"cell_type":"markdown","metadata":{"id":"5rDr8-lWn0zi"},"source":["- Add new argument in `parse_args()` function to define the repo-id where we want to push the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHQiqQEFn0QH"},"outputs":[],"source":["# Adding HuggingFace argument\n","parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")"]},{"cell_type":"markdown","metadata":{"id":"blLZMiBAoUVT"},"source":["- Next, we add the methods needed to push the model to the Hub\n","\n","- These methods will:\n","  - `_evalutate_agent()`: evaluate the agent.\n","  - `_generate_model_card()`: generate the model card of your agent.\n","  - `_record_video()`: record a video of your agent."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"WlLcz4L9odXs","executionInfo":{"status":"ok","timestamp":1694871157397,"user_tz":-540,"elapsed":326,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}}},"outputs":[],"source":["def package_to_hub(repo_id,\n","                model,\n","                hyperparameters,\n","                eval_env,\n","                video_fps=30,\n","                commit_message=\"Push agent to the Hub\",\n","                token= None,\n","                logs=None\n","                ):\n","  \"\"\"\n","  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n","  This method does the complete pipeline:\n","  - It evaluates the model\n","  - It generates the model card\n","  - It generates a replay video of the agent\n","  - It pushes everything to the hub\n","  :param repo_id: id of the model repository from the Hugging Face Hub\n","  :param model: trained model\n","  :param eval_env: environment used to evaluate the agent\n","  :param fps: number of fps for rendering the video\n","  :param commit_message: commit message\n","  :param logs: directory on local machine of tensorboard logs you'd like to upload\n","  \"\"\"\n","  msg.info(\n","        \"This function will save, evaluate, generate a video of your agent, \"\n","        \"create a model card and push everything to the hub. \"\n","        \"It might take up to 1min. \\n \"\n","        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n","    )\n","  # Step 1: Clone or create the repo\n","  repo_url = HfApi().create_repo(\n","        repo_id=repo_id,\n","        token=token,\n","        private=False,\n","        exist_ok=True,\n","    )\n","\n","  with tempfile.TemporaryDirectory() as tmpdirname:\n","    tmpdirname = Path(tmpdirname)\n","\n","    # Step 2: Save the model\n","    torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n","\n","    # Step 3: Evaluate the model and build JSON\n","    mean_reward, std_reward = _evaluate_agent(eval_env,\n","                                           10,\n","                                           model)\n","\n","    # First get datetime\n","    eval_datetime = datetime.datetime.now()\n","    eval_form_datetime = eval_datetime.isoformat()\n","\n","    evaluate_data = {\n","        \"env_id\": hyperparameters.env_id,\n","        \"mean_reward\": mean_reward,\n","        \"std_reward\": std_reward,\n","        \"n_evaluation_episodes\": 10,\n","        \"eval_datetime\": eval_form_datetime,\n","    }\n","\n","    # Write a JSON file\n","    with open(tmpdirname / \"results.json\", \"w\") as outfile:\n","      json.dump(evaluate_data, outfile)\n","\n","    # Step 4: Generate a video\n","    video_path =  tmpdirname / \"replay.mp4\"\n","    record_video(eval_env, model, video_path, video_fps)\n","\n","    # Step 5: Generate the model card\n","    generated_model_card, metadata = _generate_model_card(\"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters)\n","    _save_model_card(tmpdirname, generated_model_card, metadata)\n","\n","    # Step 6: Add logs if needed\n","    if logs:\n","      _add_logdir(tmpdirname, Path(logs))\n","\n","    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n","\n","    repo_url = upload_folder(\n","            repo_id=repo_id,\n","            folder_path=tmpdirname,\n","            path_in_repo=\"\",\n","            commit_message=commit_message,\n","            token=token,\n","        )\n","\n","    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n","  return repo_url\n","\n","\n","def _evaluate_agent(env, n_eval_episodes, policy):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param policy: The agent\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in range(n_eval_episodes):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards_ep = 0\n","\n","    while done is False:\n","      state = torch.Tensor(state).to(device)\n","      action, _, _, _ = policy.get_action_and_value(state)\n","      new_state, reward, done, info = env.step(action.cpu().numpy())\n","      total_rewards_ep += reward\n","      if done:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward\n","\n","\n","def record_video(env, policy, out_directory, fps=30):\n","  images = []\n","  done = False\n","  state = env.reset()\n","  img = env.render(mode='rgb_array')\n","  images.append(img)\n","  while not done:\n","    state = torch.Tensor(state).to(device)\n","    # Take the action (index) that have the maximum expected future reward given that state\n","    action, _, _, _  = policy.get_action_and_value(state)\n","    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n","    img = env.render(mode='rgb_array')\n","    images.append(img)\n","  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n","\n","\n","def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n","  \"\"\"\n","  Generate the model card for the Hub\n","  :param model_name: name of the model\n","  :env_id: name of the environment\n","  :mean_reward: mean reward of the agent\n","  :std_reward: standard deviation of the mean reward of the agent\n","  :hyperparameters: training arguments\n","  \"\"\"\n","  # Step 1: Select the tags\n","  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n","\n","  # Transform the hyperparams namespace to string\n","  converted_dict = vars(hyperparameters)\n","  converted_str = str(converted_dict)\n","  converted_str = converted_str.split(\", \")\n","  converted_str = '\\n'.join(converted_str)\n","\n","  # Step 2: Generate the model card\n","  model_card = f\"\"\"\n","  # PPO Agent Playing {env_id}\n","\n","  This is a trained model of a PPO agent playing {env_id}.\n","\n","  # Hyperparameters\n","  ```python\n","  {converted_str}\n","  ```\n","  \"\"\"\n","  return model_card, metadata\n","\n","\n","def generate_metadata(model_name, env_id, mean_reward, std_reward):\n","  \"\"\"\n","  Define the tags for the model card\n","  :param model_name: name of the model\n","  :param env_id: name of the environment\n","  :mean_reward: mean reward of the agent\n","  :std_reward: standard deviation of the mean reward of the agent\n","  \"\"\"\n","  metadata = {}\n","  metadata[\"tags\"] = [\n","        env_id,\n","        \"ppo\",\n","        \"deep-reinforcement-learning\",\n","        \"reinforcement-learning\",\n","        \"custom-implementation\",\n","        \"deep-rl-course\"\n","  ]\n","\n","  # Add metrics\n","  eval = metadata_eval_result(\n","      model_pretty_name=model_name,\n","      task_pretty_name=\"reinforcement-learning\",\n","      task_id=\"reinforcement-learning\",\n","      metrics_pretty_name=\"mean_reward\",\n","      metrics_id=\"mean_reward\",\n","      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n","      dataset_pretty_name=env_id,\n","      dataset_id=env_id,\n","  )\n","\n","  # Merges both dictionaries\n","  metadata = {**metadata, **eval}\n","\n","  return metadata\n","\n","\n","def _save_model_card(local_path, generated_model_card, metadata):\n","    \"\"\"Saves a model card for the repository.\n","    :param local_path: repository directory\n","    :param generated_model_card: model card generated by _generate_model_card()\n","    :param metadata: metadata\n","    \"\"\"\n","    readme_path = local_path / \"README.md\"\n","    readme = \"\"\n","    if readme_path.exists():\n","        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n","            readme = f.read()\n","    else:\n","        readme = generated_model_card\n","\n","    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n","        f.write(readme)\n","\n","    # Save our metrics to Readme metadata\n","    metadata_save(readme_path, metadata)\n","\n","\n","def _add_logdir(local_path: Path, logdir: Path):\n","  \"\"\"Adds a logdir to the repository.\n","  :param local_path: repository directory\n","  :param logdir: logdir directory\n","  \"\"\"\n","  if logdir.exists() and logdir.is_dir():\n","    # Add the logdir to the repository under new dir called logs\n","    repo_logdir = local_path / \"logs\"\n","\n","    # Delete current logs if they exist\n","    if repo_logdir.exists():\n","      shutil.rmtree(repo_logdir)\n","\n","    # Copy logdir into repo logdir\n","    shutil.copytree(logdir, repo_logdir)"]},{"cell_type":"markdown","metadata":{"id":"TqX8z8_rooD6"},"source":["- Finally, we call this function at the end of the PPO training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8V1vNiTo2hL"},"outputs":[],"source":["# Create the evaluation environment\n","eval_env = gym.make(args.env_id)\n","\n","package_to_hub(repo_id = args.repo_id,\n","                model = agent, # The model we want to save\n","                hyperparameters = args,\n","                eval_env = gym.make(args.env_id),\n","                logs= f\"runs/{run_name}\",\n","                )"]},{"cell_type":"markdown","metadata":{"id":"muCCzed4o5TC"},"source":["- Here's what look the ppo.py final file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LviRdtXgo7kF"},"outputs":[],"source":["# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy\n","\n","import argparse\n","import os\n","import random\n","import time\n","from distutils.util import strtobool\n","\n","import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions.categorical import Categorical\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from huggingface_hub import HfApi, upload_folder\n","from huggingface_hub.repocard import metadata_eval_result, metadata_save\n","\n","from pathlib import Path\n","import datetime\n","import tempfile\n","import json\n","import shutil\n","import imageio\n","\n","from wasabi import Printer\n","msg = Printer()\n","\n","def parse_args():\n","    # fmt: off\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n","        help=\"the name of this experiment\")\n","    parser.add_argument(\"--seed\", type=int, default=1,\n","        help=\"seed of the experiment\")\n","    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n","    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"if toggled, cuda will be enabled by default\")\n","    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n","        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n","    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n","        help=\"the wandb's project name\")\n","    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n","        help=\"the entity (team) of wandb's project\")\n","    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n","        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n","\n","    # Algorithm specific arguments\n","    parser.add_argument(\"--env-id\", type=str, default=\"CartPole-v1\",\n","        help=\"the id of the environment\")\n","    parser.add_argument(\"--total-timesteps\", type=int, default=50000,\n","        help=\"total timesteps of the experiments\")\n","    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n","        help=\"the learning rate of the optimizer\")\n","    parser.add_argument(\"--num-envs\", type=int, default=4,\n","        help=\"the number of parallel game environments\")\n","    parser.add_argument(\"--num-steps\", type=int, default=128,\n","        help=\"the number of steps to run in each environment per policy rollout\")\n","    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"Toggle learning rate annealing for policy and value networks\")\n","    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"Use GAE for advantage computation\")\n","    parser.add_argument(\"--gamma\", type=float, default=0.99,\n","        help=\"the discount factor gamma\")\n","    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n","        help=\"the lambda for the general advantage estimation\")\n","    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n","        help=\"the number of mini-batches\")\n","    parser.add_argument(\"--update-epochs\", type=int, default=4,\n","        help=\"the K epochs to update the policy\")\n","    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"Toggles advantages normalization\")\n","    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n","        help=\"the surrogate clipping coefficient\")\n","    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n","        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n","    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n","        help=\"coefficient of the entropy\")\n","    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n","        help=\"coefficient of the value function\")\n","    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n","        help=\"the maximum norm for the gradient clipping\")\n","    parser.add_argument(\"--target-kl\", type=float, default=None,\n","        help=\"the target KL divergence threshold\")\n","\n","    # Adding HuggingFace argument\n","    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n","\n","    args = parser.parse_args()\n","    args.batch_size = int(args.num_envs * args.num_steps)\n","    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n","    # fmt: on\n","    return args\n","\n","def package_to_hub(repo_id,\n","                model,\n","                hyperparameters,\n","                eval_env,\n","                video_fps=30,\n","                commit_message=\"Push agent to the Hub\",\n","                token= None,\n","                logs=None\n","                ):\n","  \"\"\"\n","  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n","  This method does the complete pipeline:\n","  - It evaluates the model\n","  - It generates the model card\n","  - It generates a replay video of the agent\n","  - It pushes everything to the hub\n","  :param repo_id: id of the model repository from the Hugging Face Hub\n","  :param model: trained model\n","  :param eval_env: environment used to evaluate the agent\n","  :param fps: number of fps for rendering the video\n","  :param commit_message: commit message\n","  :param logs: directory on local machine of tensorboard logs you'd like to upload\n","  \"\"\"\n","  msg.info(\n","        \"This function will save, evaluate, generate a video of your agent, \"\n","        \"create a model card and push everything to the hub. \"\n","        \"It might take up to 1min. \\n \"\n","        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n","    )\n","  # Step 1: Clone or create the repo\n","  repo_url = HfApi().create_repo(\n","        repo_id=repo_id,\n","        token=token,\n","        private=False,\n","        exist_ok=True,\n","    )\n","\n","  with tempfile.TemporaryDirectory() as tmpdirname:\n","    tmpdirname = Path(tmpdirname)\n","\n","    # Step 2: Save the model\n","    torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n","\n","    # Step 3: Evaluate the model and build JSON\n","    mean_reward, std_reward = _evaluate_agent(eval_env,\n","                                           10,\n","                                           model)\n","\n","    # First get datetime\n","    eval_datetime = datetime.datetime.now()\n","    eval_form_datetime = eval_datetime.isoformat()\n","\n","    evaluate_data = {\n","        \"env_id\": hyperparameters.env_id,\n","        \"mean_reward\": mean_reward,\n","        \"std_reward\": std_reward,\n","        \"n_evaluation_episodes\": 10,\n","        \"eval_datetime\": eval_form_datetime,\n","    }\n","\n","    # Write a JSON file\n","    with open(tmpdirname / \"results.json\", \"w\") as outfile:\n","      json.dump(evaluate_data, outfile)\n","\n","    # Step 4: Generate a video\n","    video_path =  tmpdirname / \"replay.mp4\"\n","    record_video(eval_env, model, video_path, video_fps)\n","\n","    # Step 5: Generate the model card\n","    generated_model_card, metadata = _generate_model_card(\"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters)\n","    _save_model_card(tmpdirname, generated_model_card, metadata)\n","\n","    # Step 6: Add logs if needed\n","    if logs:\n","      _add_logdir(tmpdirname, Path(logs))\n","\n","    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n","\n","    repo_url = upload_folder(\n","            repo_id=repo_id,\n","            folder_path=tmpdirname,\n","            path_in_repo=\"\",\n","            commit_message=commit_message,\n","            token=token,\n","        )\n","\n","    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n","  return repo_url\n","\n","def _evaluate_agent(env, n_eval_episodes, policy):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param policy: The agent\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in range(n_eval_episodes):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards_ep = 0\n","\n","    while done is False:\n","      state = torch.Tensor(state).to(device)\n","      action, _, _, _ = policy.get_action_and_value(state)\n","      new_state, reward, done, info = env.step(action.cpu().numpy())\n","      total_rewards_ep += reward\n","      if done:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward\n","\n","\n","def record_video(env, policy, out_directory, fps=30):\n","  images = []\n","  done = False\n","  state = env.reset()\n","  img = env.render(mode='rgb_array')\n","  images.append(img)\n","  while not done:\n","    state = torch.Tensor(state).to(device)\n","    # Take the action (index) that have the maximum expected future reward given that state\n","    action, _, _, _  = policy.get_action_and_value(state)\n","    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n","    img = env.render(mode='rgb_array')\n","    images.append(img)\n","  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n","\n","\n","def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n","  \"\"\"\n","  Generate the model card for the Hub\n","  :param model_name: name of the model\n","  :env_id: name of the environment\n","  :mean_reward: mean reward of the agent\n","  :std_reward: standard deviation of the mean reward of the agent\n","  :hyperparameters: training arguments\n","  \"\"\"\n","  # Step 1: Select the tags\n","  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n","\n","  # Transform the hyperparams namespace to string\n","  converted_dict = vars(hyperparameters)\n","  converted_str = str(converted_dict)\n","  converted_str = converted_str.split(\", \")\n","  converted_str = '\\n'.join(converted_str)\n","\n","  # Step 2: Generate the model card\n","  model_card = f\"\"\"\n","  # PPO Agent Playing {env_id}\n","\n","  This is a trained model of a PPO agent playing {env_id}.\n","\n","  # Hyperparameters\n","  ```python\n","  {converted_str}\n","  ```\n","  \"\"\"\n","  return model_card, metadata\n","\n","def generate_metadata(model_name, env_id, mean_reward, std_reward):\n","  \"\"\"\n","  Define the tags for the model card\n","  :param model_name: name of the model\n","  :param env_id: name of the environment\n","  :mean_reward: mean reward of the agent\n","  :std_reward: standard deviation of the mean reward of the agent\n","  \"\"\"\n","  metadata = {}\n","  metadata[\"tags\"] = [\n","        env_id,\n","        \"ppo\",\n","        \"deep-reinforcement-learning\",\n","        \"reinforcement-learning\",\n","        \"custom-implementation\",\n","        \"deep-rl-course\"\n","  ]\n","\n","  # Add metrics\n","  eval = metadata_eval_result(\n","      model_pretty_name=model_name,\n","      task_pretty_name=\"reinforcement-learning\",\n","      task_id=\"reinforcement-learning\",\n","      metrics_pretty_name=\"mean_reward\",\n","      metrics_id=\"mean_reward\",\n","      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n","      dataset_pretty_name=env_id,\n","      dataset_id=env_id,\n","  )\n","\n","  # Merges both dictionaries\n","  metadata = {**metadata, **eval}\n","\n","  return metadata\n","\n","def _save_model_card(local_path, generated_model_card, metadata):\n","    \"\"\"Saves a model card for the repository.\n","    :param local_path: repository directory\n","    :param generated_model_card: model card generated by _generate_model_card()\n","    :param metadata: metadata\n","    \"\"\"\n","    readme_path = local_path / \"README.md\"\n","    readme = \"\"\n","    if readme_path.exists():\n","        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n","            readme = f.read()\n","    else:\n","        readme = generated_model_card\n","\n","    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n","        f.write(readme)\n","\n","    # Save our metrics to Readme metadata\n","    metadata_save(readme_path, metadata)\n","\n","def _add_logdir(local_path: Path, logdir: Path):\n","  \"\"\"Adds a logdir to the repository.\n","  :param local_path: repository directory\n","  :param logdir: logdir directory\n","  \"\"\"\n","  if logdir.exists() and logdir.is_dir():\n","    # Add the logdir to the repository under new dir called logs\n","    repo_logdir = local_path / \"logs\"\n","\n","    # Delete current logs if they exist\n","    if repo_logdir.exists():\n","      shutil.rmtree(repo_logdir)\n","\n","    # Copy logdir into repo logdir\n","    shutil.copytree(logdir, repo_logdir)\n","\n","def make_env(env_id, seed, idx, capture_video, run_name):\n","    def thunk():\n","        env = gym.make(env_id)\n","        env = gym.wrappers.RecordEpisodeStatistics(env)\n","        if capture_video:\n","            if idx == 0:\n","                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n","        env.seed(seed)\n","        env.action_space.seed(seed)\n","        env.observation_space.seed(seed)\n","        return env\n","\n","    return thunk\n","\n","\n","def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n","    torch.nn.init.orthogonal_(layer.weight, std)\n","    torch.nn.init.constant_(layer.bias, bias_const)\n","    return layer\n","\n","\n","class Agent(nn.Module):\n","    def __init__(self, envs):\n","        super().__init__()\n","        self.critic = nn.Sequential(\n","            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 1), std=1.0),\n","        )\n","        self.actor = nn.Sequential(\n","            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, 64)),\n","            nn.Tanh(),\n","            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n","        )\n","\n","    def get_value(self, x):\n","        return self.critic(x)\n","\n","    def get_action_and_value(self, x, action=None):\n","        logits = self.actor(x)\n","        probs = Categorical(logits=logits)\n","        if action is None:\n","            action = probs.sample()\n","        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n","\n","\n","if __name__ == \"__main__\":\n","    args = parse_args()\n","    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n","    if args.track:\n","        import wandb\n","\n","        wandb.init(\n","            project=args.wandb_project_name,\n","            entity=args.wandb_entity,\n","            sync_tensorboard=True,\n","            config=vars(args),\n","            name=run_name,\n","            monitor_gym=True,\n","            save_code=True,\n","        )\n","    writer = SummaryWriter(f\"runs/{run_name}\")\n","    writer.add_text(\n","        \"hyperparameters\",\n","        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n","    )\n","\n","    # TRY NOT TO MODIFY: seeding\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","\n","    # env setup\n","    envs = gym.vector.SyncVectorEnv(\n","        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n","    )\n","    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n","\n","    agent = Agent(envs).to(device)\n","    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n","\n","    # ALGO Logic: Storage setup\n","    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n","    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n","    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","\n","    # TRY NOT TO MODIFY: start the game\n","    global_step = 0\n","    start_time = time.time()\n","    next_obs = torch.Tensor(envs.reset()).to(device)\n","    next_done = torch.zeros(args.num_envs).to(device)\n","    num_updates = args.total_timesteps // args.batch_size\n","\n","    for update in range(1, num_updates + 1):\n","        # Annealing the rate if instructed to do so.\n","        if args.anneal_lr:\n","            frac = 1.0 - (update - 1.0) / num_updates\n","            lrnow = frac * args.learning_rate\n","            optimizer.param_groups[0][\"lr\"] = lrnow\n","\n","        for step in range(0, args.num_steps):\n","            global_step += 1 * args.num_envs\n","            obs[step] = next_obs\n","            dones[step] = next_done\n","\n","            # ALGO LOGIC: action logic\n","            with torch.no_grad():\n","                action, logprob, _, value = agent.get_action_and_value(next_obs)\n","                values[step] = value.flatten()\n","            actions[step] = action\n","            logprobs[step] = logprob\n","\n","            # TRY NOT TO MODIFY: execute the game and log data.\n","            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n","            rewards[step] = torch.tensor(reward).to(device).view(-1)\n","            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n","\n","            for item in info:\n","                if \"episode\" in item.keys():\n","                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n","                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n","                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n","                    break\n","\n","        # bootstrap value if not done\n","        with torch.no_grad():\n","            next_value = agent.get_value(next_obs).reshape(1, -1)\n","            if args.gae:\n","                advantages = torch.zeros_like(rewards).to(device)\n","                lastgaelam = 0\n","                for t in reversed(range(args.num_steps)):\n","                    if t == args.num_steps - 1:\n","                        nextnonterminal = 1.0 - next_done\n","                        nextvalues = next_value\n","                    else:\n","                        nextnonterminal = 1.0 - dones[t + 1]\n","                        nextvalues = values[t + 1]\n","                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n","                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n","                returns = advantages + values\n","            else:\n","                returns = torch.zeros_like(rewards).to(device)\n","                for t in reversed(range(args.num_steps)):\n","                    if t == args.num_steps - 1:\n","                        nextnonterminal = 1.0 - next_done\n","                        next_return = next_value\n","                    else:\n","                        nextnonterminal = 1.0 - dones[t + 1]\n","                        next_return = returns[t + 1]\n","                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n","                advantages = returns - values\n","\n","        # flatten the batch\n","        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n","        b_logprobs = logprobs.reshape(-1)\n","        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n","        b_advantages = advantages.reshape(-1)\n","        b_returns = returns.reshape(-1)\n","        b_values = values.reshape(-1)\n","\n","        # Optimizing the policy and value network\n","        b_inds = np.arange(args.batch_size)\n","        clipfracs = []\n","        for epoch in range(args.update_epochs):\n","            np.random.shuffle(b_inds)\n","            for start in range(0, args.batch_size, args.minibatch_size):\n","                end = start + args.minibatch_size\n","                mb_inds = b_inds[start:end]\n","\n","                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n","                logratio = newlogprob - b_logprobs[mb_inds]\n","                ratio = logratio.exp()\n","\n","                with torch.no_grad():\n","                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n","                    old_approx_kl = (-logratio).mean()\n","                    approx_kl = ((ratio - 1) - logratio).mean()\n","                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n","\n","                mb_advantages = b_advantages[mb_inds]\n","                if args.norm_adv:\n","                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n","\n","                # Policy loss\n","                pg_loss1 = -mb_advantages * ratio\n","                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n","                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n","\n","                # Value loss\n","                newvalue = newvalue.view(-1)\n","                if args.clip_vloss:\n","                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n","                    v_clipped = b_values[mb_inds] + torch.clamp(\n","                        newvalue - b_values[mb_inds],\n","                        -args.clip_coef,\n","                        args.clip_coef,\n","                    )\n","                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n","                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n","                    v_loss = 0.5 * v_loss_max.mean()\n","                else:\n","                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n","\n","                entropy_loss = entropy.mean()\n","                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n","                optimizer.step()\n","\n","            if args.target_kl is not None:\n","                if approx_kl > args.target_kl:\n","                    break\n","\n","        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n","        var_y = np.var(y_true)\n","        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n","\n","        # TRY NOT TO MODIFY: record rewards for plotting purposes\n","        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n","        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n","        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n","        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n","        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n","        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n","        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n","        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n","        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n","        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n","\n","    envs.close()\n","    writer.close()\n","\n","    # Create the evaluation environment\n","    eval_env = gym.make(args.env_id)\n","\n","    package_to_hub(repo_id = args.repo_id,\n","                model = agent, # The model we want to save\n","                hyperparameters = args,\n","                eval_env = gym.make(args.env_id),\n","                logs= f\"runs/{run_name}\",\n","                )\n"]},{"cell_type":"markdown","metadata":{"id":"JquRrWytA6eo"},"source":["To be able to share your model with the community there are three more steps to follow:\n","\n","1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n","\n","2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n","- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n","\n","- Copy the token\n","- Run the cell below and paste the token"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"GZiFBBlzxzxY","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["52c7dd910ca640f78825e19534953c27","6d52daec44c340eeb49b58a8da65dca7","c9ec0f72f8b44456a7f86c2035cc107c","e9094548b1fe448cbbda858c8b7565f5","d33d91c430504cf49be1bfab95b08279","2e5376eb4ab64230a2a5fbe8ba806ff1","f5b80e76e63645e59a909c0b1bd9dc1d","8b96c87489a44700a41b94fa4fd94386","e18f717d509f4a90b61f1703274684c7","f48df99ebe894267b91e981f72733195","45e51deaff344f5cab945e05c9bf4f16","9f2d2e1f311448fb9f9cc81fdbc2de3a","655571ac35eb4f44a1274d2def38e9f1","aba645507b654bd9ad7020163bf6f032","4399f266eb5441c29e8c7c3dd113e52e","848b344d72254a3c9dc5072f13f417b5","0013a08a3d0246a68ab7102157331918","579e5d9d0dbd4d8abfaf0a3c0acc56e4","828c6f75928b47708578f9f419848651","c112f2d4b3494caf85b17571dc94fc8f","72f563c9e5864344b42b6a3c75d51628","54ec2df88de2410c83bfbd36dfe02af8","141b3eb578b842c8971ea9fc34b8c63a","dbcf2e3ef5dd4bd99c5dc23e9d20ee8e","44203070f32a4bc6a7cfc386f601ea70","16e1eff35c554617b339b1bdd0689973","1d9d97f9878a436483d242eb8f90abf8","ebc603fcd0244850ac6ca40ac0a16f12","71007af36dae4de5a06e7ce404478014","f6b1354b01874e15bd66c56c45b85d9e","b563dd0886f34242bb3b45b3daebc445","144c6ed281e040d09f0675d8536ada3a"]},"executionInfo":{"status":"ok","timestamp":1695096916458,"user_tz":-540,"elapsed":23,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}},"outputId":"6df2cecc-0420-4584-86c1-58e0ad192b64"},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c7dd910ca640f78825e19534953c27"}},"metadata":{}}],"source":["from huggingface_hub import notebook_login\n","notebook_login()\n","!git config --global credential.helper store"]},{"cell_type":"markdown","metadata":{"id":"_tsf2uv0g_4p"},"source":["If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"]},{"cell_type":"markdown","metadata":{"id":"jRqkGvk7pFQ6"},"source":["## Let's start the training üî•\n","- ‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è  Don't use **the same repo id with the one you used for the Unit 1**\n","- Now that you've coded from scratch PPO and added the Hugging Face Integration, we're ready to start the training üî•"]},{"cell_type":"markdown","metadata":{"id":"0tmEArP8ug2l"},"source":["- First, you need to copy all your code to a file you create called `ppo.py`"]},{"cell_type":"markdown","source":["<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/step1.png\" alt=\"PPO\"/>"],"metadata":{"id":"Sq0My0LOjPYR"}},{"cell_type":"markdown","source":["<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/step2.png\" alt=\"PPO\"/>"],"metadata":{"id":"A8C-Q5ZyjUe3"}},{"cell_type":"markdown","metadata":{"id":"VrS80GmMu_j5"},"source":["- Now we just need to run this python script using `python <name-of-python-script>.py` with the additional parameters we defined with `argparse`\n","\n","- You should modify more hyperparameters otherwise the training will not be super stable."]},{"cell_type":"code","source":["!python my_ppo.py --env-id=\"LunarLander-v2\" --repo-id=\"BanUrsus/my-ppo-LunarLander-v2\" --total-timesteps=500000"],"metadata":{"id":"KXLih6mKseBs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695094988258,"user_tz":-540,"elapsed":860449,"user":{"displayName":"Î∞òÎã¨Í≥∞","userId":"09517991737165413775"}},"outputId":"c50efc74-27fb-4eab-a344-df620b54d14d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","2023-09-19 04:03:36.846452: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-09-19 04:03:38.333798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","global_step=424, episodic_return=-170.81808471679688\n","global_step=472, episodic_return=-320.47113037109375\n","global_step=476, episodic_return=-81.00199127197266\n","global_step=484, episodic_return=-20.284828186035156\n","SPS: 556\n","global_step=700, episodic_return=-95.84423828125\n","global_step=784, episodic_return=-81.619140625\n","global_step=844, episodic_return=-123.97432708740234\n","global_step=948, episodic_return=-176.86416625976562\n","global_step=1000, episodic_return=-130.58908081054688\n","SPS: 803\n","global_step=1136, episodic_return=-112.11347198486328\n","global_step=1188, episodic_return=-513.12451171875\n","global_step=1236, episodic_return=-97.30636596679688\n","global_step=1352, episodic_return=-254.072265625\n","global_step=1404, episodic_return=-155.40774536132812\n","SPS: 951\n","global_step=1624, episodic_return=-161.66262817382812\n","global_step=1712, episodic_return=-250.63983154296875\n","global_step=1752, episodic_return=-285.36212158203125\n","global_step=1816, episodic_return=-151.1981964111328\n","global_step=1888, episodic_return=-66.41668701171875\n","global_step=2004, episodic_return=-85.48785400390625\n","SPS: 1037\n","global_step=2080, episodic_return=-77.71797180175781\n","global_step=2240, episodic_return=-95.5638198852539\n","global_step=2276, episodic_return=-68.39567565917969\n","global_step=2416, episodic_return=-186.36868286132812\n","global_step=2540, episodic_return=-256.4453125\n","global_step=2548, episodic_return=-122.00627136230469\n","SPS: 1095\n","global_step=2624, episodic_return=-289.4886169433594\n","global_step=2760, episodic_return=-217.73216247558594\n","global_step=2916, episodic_return=-143.02392578125\n","global_step=2956, episodic_return=-105.69833374023438\n","global_step=3040, episodic_return=-78.09066772460938\n","SPS: 1145\n","global_step=3116, episodic_return=-351.5619812011719\n","global_step=3184, episodic_return=-99.38037109375\n","global_step=3432, episodic_return=-372.6194152832031\n","global_step=3436, episodic_return=-100.83786010742188\n","SPS: 1177\n","global_step=3588, episodic_return=-391.9200744628906\n","global_step=3612, episodic_return=-206.20970153808594\n","global_step=3828, episodic_return=-125.66621398925781\n","global_step=3904, episodic_return=-150.24061584472656\n","global_step=3992, episodic_return=-126.70933532714844\n","global_step=4000, episodic_return=-302.3304748535156\n","SPS: 1204\n","global_step=4204, episodic_return=-140.3133544921875\n","global_step=4268, episodic_return=-63.64165115356445\n","global_step=4360, episodic_return=-460.6656799316406\n","global_step=4384, episodic_return=-231.4110107421875\n","global_step=4556, episodic_return=-122.8847427368164\n","global_step=4592, episodic_return=-118.49877166748047\n","SPS: 1228\n","global_step=4716, episodic_return=-423.15472412109375\n","global_step=4836, episodic_return=-69.0478744506836\n","global_step=4892, episodic_return=-111.45392608642578\n","global_step=4936, episodic_return=-152.482177734375\n","SPS: 1246\n","global_step=5144, episodic_return=-58.405662536621094\n","global_step=5148, episodic_return=-346.6878967285156\n","global_step=5204, episodic_return=-219.98391723632812\n","global_step=5396, episodic_return=-520.898681640625\n","global_step=5452, episodic_return=-103.41637420654297\n","global_step=5456, episodic_return=-213.93658447265625\n","global_step=5580, episodic_return=-98.23206329345703\n","SPS: 1259\n","global_step=5796, episodic_return=-119.69186401367188\n","global_step=5824, episodic_return=-49.75025939941406\n","global_step=5868, episodic_return=-334.07501220703125\n","global_step=6080, episodic_return=-116.8465576171875\n","global_step=6116, episodic_return=-181.0447998046875\n","global_step=6132, episodic_return=-89.88493347167969\n","SPS: 1266\n","global_step=6196, episodic_return=-131.8802947998047\n","global_step=6384, episodic_return=-83.94912719726562\n","global_step=6464, episodic_return=-73.17933654785156\n","global_step=6596, episodic_return=-256.5047912597656\n","global_step=6632, episodic_return=-104.91091918945312\n","SPS: 1277\n","global_step=6712, episodic_return=-84.6820297241211\n","global_step=6740, episodic_return=-150.40518188476562\n","global_step=6904, episodic_return=-74.46856689453125\n","global_step=7060, episodic_return=-132.00880432128906\n","global_step=7116, episodic_return=-131.25308227539062\n","global_step=7152, episodic_return=-137.2035675048828\n","SPS: 1291\n","global_step=7348, episodic_return=-108.9298324584961\n","global_step=7368, episodic_return=-92.64199829101562\n","global_step=7432, episodic_return=-84.7559585571289\n","global_step=7636, episodic_return=-52.73688888549805\n","global_step=7652, episodic_return=-83.99880981445312\n","SPS: 1305\n","global_step=7716, episodic_return=-164.3382568359375\n","global_step=7844, episodic_return=-114.04733276367188\n","global_step=8032, episodic_return=-202.57691955566406\n","global_step=8048, episodic_return=-241.86776733398438\n","global_step=8116, episodic_return=-93.10206604003906\n","global_step=8148, episodic_return=-144.21009826660156\n","SPS: 1314\n","global_step=8360, episodic_return=-82.00491333007812\n","global_step=8372, episodic_return=-101.88496398925781\n","global_step=8468, episodic_return=-296.43731689453125\n","global_step=8500, episodic_return=-164.40362548828125\n","global_step=8616, episodic_return=-154.96267700195312\n","SPS: 1324\n","global_step=8712, episodic_return=-341.05560302734375\n","global_step=8776, episodic_return=-76.67391967773438\n","global_step=8828, episodic_return=-74.16917419433594\n","global_step=8844, episodic_return=-215.18917846679688\n","global_step=9204, episodic_return=-132.03932189941406\n","SPS: 1303\n","global_step=9224, episodic_return=-94.32894134521484\n","global_step=9252, episodic_return=-95.8163833618164\n","global_step=9532, episodic_return=-149.20556640625\n","global_step=9580, episodic_return=-270.64276123046875\n","global_step=9672, episodic_return=-347.0939025878906\n","SPS: 1290\n","global_step=9800, episodic_return=-163.584228515625\n","global_step=9832, episodic_return=-342.90631103515625\n","global_step=9900, episodic_return=-74.39653778076172\n","global_step=10008, episodic_return=-128.95513916015625\n","global_step=10044, episodic_return=-81.92312622070312\n","global_step=10096, episodic_return=-94.14427185058594\n","global_step=10168, episodic_return=-90.53622436523438\n","SPS: 1280\n","global_step=10336, episodic_return=-106.86229705810547\n","global_step=10352, episodic_return=-89.40869903564453\n","global_step=10592, episodic_return=-97.59760284423828\n","global_step=10608, episodic_return=-153.76734924316406\n","SPS: 1267\n","global_step=10780, episodic_return=-522.2782592773438\n","global_step=10876, episodic_return=-60.950836181640625\n","global_step=10928, episodic_return=-112.46768188476562\n","global_step=11116, episodic_return=-106.3084945678711\n","global_step=11216, episodic_return=-387.47015380859375\n","SPS: 1253\n","global_step=11356, episodic_return=-162.37843322753906\n","global_step=11368, episodic_return=-175.67001342773438\n","global_step=11500, episodic_return=-355.69879150390625\n","global_step=11564, episodic_return=-504.36004638671875\n","global_step=11704, episodic_return=-68.49559020996094\n","SPS: 1237\n","global_step=11812, episodic_return=-274.6031799316406\n","global_step=11960, episodic_return=-137.82473754882812\n","global_step=11968, episodic_return=-91.3621597290039\n","global_step=12132, episodic_return=-3.693206787109375\n","global_step=12168, episodic_return=-241.74952697753906\n","SPS: 1243\n","global_step=12352, episodic_return=-71.90823364257812\n","global_step=12388, episodic_return=-163.5650177001953\n","global_step=12468, episodic_return=-126.02500915527344\n","global_step=12528, episodic_return=-119.83267211914062\n","global_step=12652, episodic_return=-127.81233215332031\n","global_step=12656, episodic_return=-104.85663604736328\n","global_step=12792, episodic_return=-106.7710189819336\n","global_step=12800, episodic_return=-58.74791717529297\n","SPS: 1232\n","global_step=13036, episodic_return=-122.38243103027344\n","global_step=13136, episodic_return=-120.36595153808594\n","global_step=13256, episodic_return=-270.9796142578125\n","global_step=13308, episodic_return=-72.18486785888672\n","SPS: 1238\n","global_step=13408, episodic_return=-207.48117065429688\n","global_step=13528, episodic_return=-143.40614318847656\n","global_step=13560, episodic_return=-85.7740478515625\n","global_step=13700, episodic_return=-108.7951889038086\n","global_step=13772, episodic_return=-92.93716430664062\n","global_step=13792, episodic_return=-131.59024047851562\n","SPS: 1244\n","global_step=13936, episodic_return=-67.27191925048828\n","global_step=14136, episodic_return=-191.6309814453125\n","global_step=14144, episodic_return=-129.22312927246094\n","global_step=14208, episodic_return=-155.05654907226562\n","global_step=14228, episodic_return=-241.44522094726562\n","SPS: 1251\n","global_step=14432, episodic_return=-62.42844772338867\n","global_step=14556, episodic_return=-130.5467987060547\n","global_step=14648, episodic_return=-156.86349487304688\n","global_step=14676, episodic_return=-222.27212524414062\n","SPS: 1255\n","global_step=14964, episodic_return=-215.4966278076172\n","global_step=15020, episodic_return=-164.43882751464844\n","global_step=15100, episodic_return=-97.94883728027344\n","global_step=15180, episodic_return=0.8876266479492188\n","global_step=15296, episodic_return=-72.35720825195312\n","SPS: 1259\n","global_step=15380, episodic_return=-75.64607238769531\n","global_step=15384, episodic_return=-98.46650695800781\n","global_step=15648, episodic_return=-246.087646484375\n","global_step=15752, episodic_return=-281.4412841796875\n","global_step=15844, episodic_return=-104.89128875732422\n","SPS: 1264\n","global_step=15944, episodic_return=-11.907524108886719\n","global_step=16148, episodic_return=-84.88605499267578\n","global_step=16156, episodic_return=-255.79339599609375\n","global_step=16284, episodic_return=-163.15476989746094\n","global_step=16320, episodic_return=-101.21267700195312\n","global_step=16380, episodic_return=-94.34700775146484\n","SPS: 1265\n","global_step=16596, episodic_return=-123.13525390625\n","global_step=16824, episodic_return=-360.57940673828125\n","global_step=16860, episodic_return=-241.0326385498047\n","global_step=16880, episodic_return=-197.24502563476562\n","SPS: 1268\n","global_step=16964, episodic_return=-148.6044464111328\n","global_step=17092, episodic_return=-64.69294738769531\n","global_step=17144, episodic_return=-123.32041931152344\n","global_step=17240, episodic_return=-179.23736572265625\n","global_step=17340, episodic_return=-174.2245330810547\n","SPS: 1272\n","global_step=17452, episodic_return=-45.803733825683594\n","global_step=17484, episodic_return=-126.03530883789062\n","global_step=17588, episodic_return=-81.59698486328125\n","global_step=17704, episodic_return=-77.03839111328125\n","global_step=17752, episodic_return=-92.24239349365234\n","SPS: 1278\n","global_step=17940, episodic_return=-92.52312469482422\n","global_step=17972, episodic_return=-263.4765625\n","global_step=18200, episodic_return=-132.64402770996094\n","global_step=18252, episodic_return=-98.37667846679688\n","global_step=18272, episodic_return=-115.5411376953125\n","global_step=18320, episodic_return=-153.56216430664062\n","SPS: 1280\n","global_step=18516, episodic_return=-50.10930252075195\n","global_step=18684, episodic_return=-25.007164001464844\n","global_step=18712, episodic_return=-120.42781829833984\n","global_step=18820, episodic_return=-194.763916015625\n","global_step=18908, episodic_return=-120.41789245605469\n","SPS: 1284\n","global_step=18976, episodic_return=-59.307350158691406\n","global_step=19200, episodic_return=-240.2741241455078\n","global_step=19212, episodic_return=-102.4784164428711\n","global_step=19220, episodic_return=-49.88285446166992\n","global_step=19392, episodic_return=-149.60690307617188\n","SPS: 1287\n","global_step=19604, episodic_return=-117.52151489257812\n","global_step=19624, episodic_return=-85.73524475097656\n","global_step=19716, episodic_return=-339.21307373046875\n","global_step=19732, episodic_return=-309.89105224609375\n","global_step=19948, episodic_return=-111.61683654785156\n","SPS: 1289\n","global_step=20024, episodic_return=-408.39599609375\n","global_step=20056, episodic_return=-132.18858337402344\n","global_step=20164, episodic_return=-241.1177215576172\n","global_step=20248, episodic_return=-76.4299545288086\n","global_step=20420, episodic_return=-140.66827392578125\n","global_step=20448, episodic_return=-108.43921661376953\n","SPS: 1293\n","global_step=20532, episodic_return=-79.21064758300781\n","global_step=20576, episodic_return=-254.0934295654297\n","global_step=20820, episodic_return=-94.57099151611328\n","global_step=20864, episodic_return=21.18114471435547\n","global_step=20912, episodic_return=-206.28314208984375\n","global_step=20984, episodic_return=-110.80531311035156\n","SPS: 1295\n","global_step=21124, episodic_return=-97.27874755859375\n","global_step=21176, episodic_return=-111.0921630859375\n","global_step=21416, episodic_return=-136.15008544921875\n","global_step=21444, episodic_return=-395.44512939453125\n","SPS: 1298\n","global_step=21512, episodic_return=-371.5484924316406\n","global_step=21828, episodic_return=-165.89984130859375\n","global_step=21968, episodic_return=-136.66867065429688\n","global_step=22000, episodic_return=-158.45108032226562\n","global_step=22016, episodic_return=-209.1701202392578\n","SPS: 1301\n","global_step=22076, episodic_return=-121.63482666015625\n","global_step=22276, episodic_return=-89.40550231933594\n","global_step=22328, episodic_return=-78.44178771972656\n","global_step=22332, episodic_return=-42.321807861328125\n","SPS: 1304\n","global_step=22660, episodic_return=-69.36019897460938\n","global_step=22704, episodic_return=-56.865718841552734\n","global_step=22768, episodic_return=-95.224609375\n","global_step=22848, episodic_return=-209.93299865722656\n","global_step=22984, episodic_return=-80.16656494140625\n","SPS: 1306\n","global_step=23044, episodic_return=-173.228759765625\n","global_step=23216, episodic_return=-206.0166473388672\n","global_step=23248, episodic_return=-314.75311279296875\n","global_step=23388, episodic_return=-155.5769500732422\n","SPS: 1308\n","global_step=23576, episodic_return=-94.90306091308594\n","global_step=23704, episodic_return=-118.87060546875\n","global_step=23708, episodic_return=-6.0768585205078125\n","global_step=23748, episodic_return=-120.96687316894531\n","global_step=23952, episodic_return=-96.90898132324219\n","global_step=24044, episodic_return=-98.34161376953125\n","global_step=24056, episodic_return=-116.09854125976562\n","SPS: 1309\n","global_step=24208, episodic_return=-177.36636352539062\n","global_step=24368, episodic_return=-120.03961181640625\n","global_step=24432, episodic_return=-262.67572021484375\n","global_step=24460, episodic_return=-74.60892486572266\n","global_step=24524, episodic_return=-297.3596496582031\n","SPS: 1312\n","global_step=24740, episodic_return=-206.6035919189453\n","global_step=24808, episodic_return=-123.63390350341797\n","global_step=24864, episodic_return=4.228668212890625\n","global_step=25080, episodic_return=-112.515380859375\n","SPS: 1314\n","global_step=25092, episodic_return=-128.0649871826172\n","global_step=25252, episodic_return=-133.54574584960938\n","global_step=25360, episodic_return=-162.19464111328125\n","global_step=25384, episodic_return=-116.88626861572266\n","global_step=25492, episodic_return=-143.4515838623047\n","SPS: 1316\n","global_step=25644, episodic_return=-81.2786865234375\n","global_step=25664, episodic_return=-58.432891845703125\n","global_step=25888, episodic_return=-115.86849212646484\n","global_step=25960, episodic_return=-192.2904052734375\n","global_step=25964, episodic_return=-112.16728210449219\n","SPS: 1310\n","global_step=26148, episodic_return=-186.67288208007812\n","global_step=26224, episodic_return=-64.6598892211914\n","global_step=26236, episodic_return=-132.14364624023438\n","global_step=26308, episodic_return=-109.51153564453125\n","global_step=26560, episodic_return=-70.1741943359375\n","global_step=26620, episodic_return=-68.02468872070312\n","SPS: 1303\n","global_step=26684, episodic_return=-82.89923858642578\n","global_step=26792, episodic_return=-147.90966796875\n","global_step=26916, episodic_return=-73.38811492919922\n","global_step=26964, episodic_return=20.581085205078125\n","global_step=27012, episodic_return=-228.26597595214844\n","SPS: 1298\n","global_step=27172, episodic_return=-134.655517578125\n","global_step=27300, episodic_return=24.605674743652344\n","global_step=27312, episodic_return=-119.78465270996094\n","global_step=27472, episodic_return=-328.2612609863281\n","global_step=27600, episodic_return=-48.228492736816406\n","SPS: 1291\n","global_step=27736, episodic_return=-269.9961853027344\n","global_step=27760, episodic_return=-95.57762908935547\n","global_step=27944, episodic_return=22.117042541503906\n","global_step=27976, episodic_return=-99.03023529052734\n","global_step=28040, episodic_return=-67.29304504394531\n","SPS: 1284\n","global_step=28172, episodic_return=11.426589965820312\n","global_step=28320, episodic_return=-58.56724548339844\n","global_step=28348, episodic_return=-158.04348754882812\n","global_step=28444, episodic_return=-85.33055877685547\n","global_step=28460, episodic_return=-49.73775863647461\n","SPS: 1274\n","global_step=28796, episodic_return=-82.18254089355469\n","global_step=28808, episodic_return=-123.61597442626953\n","global_step=28916, episodic_return=-67.00023651123047\n","global_step=28924, episodic_return=-179.3937530517578\n","global_step=29172, episodic_return=-57.33653259277344\n","SPS: 1276\n","global_step=29204, episodic_return=-65.88648986816406\n","global_step=29272, episodic_return=-95.25016021728516\n","global_step=29292, episodic_return=-119.03075408935547\n","global_step=29444, episodic_return=-103.92506408691406\n","global_step=29540, episodic_return=-69.24560546875\n","global_step=29600, episodic_return=-331.6649169921875\n","global_step=29608, episodic_return=-258.576171875\n","SPS: 1279\n","global_step=29748, episodic_return=-144.3280029296875\n","global_step=29892, episodic_return=-138.24942016601562\n","global_step=29940, episodic_return=-77.8497543334961\n","global_step=30008, episodic_return=-306.19171142578125\n","global_step=30108, episodic_return=-195.30014038085938\n","SPS: 1281\n","global_step=30264, episodic_return=-116.51558685302734\n","global_step=30424, episodic_return=-90.76673126220703\n","global_step=30496, episodic_return=-56.2904167175293\n","global_step=30620, episodic_return=-222.7095947265625\n","global_step=30624, episodic_return=-31.5400390625\n","SPS: 1283\n","global_step=30800, episodic_return=-89.43507385253906\n","global_step=30892, episodic_return=-93.65727996826172\n","global_step=31140, episodic_return=-199.455810546875\n","global_step=31200, episodic_return=-87.80391693115234\n","global_step=31228, episodic_return=-118.94013214111328\n","SPS: 1285\n","global_step=31256, episodic_return=-101.19617462158203\n","global_step=31500, episodic_return=-59.83613586425781\n","global_step=31516, episodic_return=-101.96501159667969\n","global_step=31592, episodic_return=-198.80075073242188\n","SPS: 1287\n","global_step=31772, episodic_return=1.264312744140625\n","global_step=31928, episodic_return=-204.39590454101562\n","global_step=31992, episodic_return=-88.62015533447266\n","global_step=32092, episodic_return=-57.54339599609375\n","global_step=32104, episodic_return=-260.056640625\n","SPS: 1289\n","global_step=32376, episodic_return=-201.04315185546875\n","global_step=32416, episodic_return=-90.81466674804688\n","global_step=32560, episodic_return=-215.47885131835938\n","global_step=32608, episodic_return=-117.0015869140625\n","SPS: 1291\n","global_step=32844, episodic_return=-54.7727165222168\n","global_step=32876, episodic_return=-139.86077880859375\n","global_step=32992, episodic_return=-121.32630920410156\n","global_step=33068, episodic_return=-176.34359741210938\n","global_step=33176, episodic_return=-87.61873626708984\n","global_step=33236, episodic_return=-82.36140441894531\n","SPS: 1292\n","global_step=33428, episodic_return=-169.7405242919922\n","global_step=33480, episodic_return=-83.84195709228516\n","global_step=33568, episodic_return=-79.60506439208984\n","global_step=33628, episodic_return=-106.54631805419922\n","SPS: 1293\n","global_step=33860, episodic_return=-107.9247817993164\n","global_step=34012, episodic_return=8.564903259277344\n","global_step=34104, episodic_return=-284.334716796875\n","global_step=34192, episodic_return=-236.60116577148438\n","SPS: 1294\n","global_step=34380, episodic_return=-110.90469360351562\n","global_step=34388, episodic_return=-30.37896728515625\n","global_step=34620, episodic_return=-104.36687469482422\n","global_step=34728, episodic_return=-211.6331787109375\n","SPS: 1297\n","global_step=34836, episodic_return=-411.50146484375\n","global_step=34876, episodic_return=-116.21472930908203\n","global_step=35112, episodic_return=-70.84855651855469\n","global_step=35172, episodic_return=-330.99053955078125\n","global_step=35272, episodic_return=-300.448974609375\n","global_step=35308, episodic_return=-174.90896606445312\n","SPS: 1298\n","global_step=35596, episodic_return=-152.2156982421875\n","global_step=35604, episodic_return=-31.517730712890625\n","global_step=35636, episodic_return=-215.59762573242188\n","global_step=35656, episodic_return=-40.320438385009766\n","SPS: 1299\n","global_step=35924, episodic_return=-67.75981140136719\n","global_step=35972, episodic_return=-35.959922790527344\n","global_step=36152, episodic_return=-91.23178100585938\n","global_step=36276, episodic_return=-82.85525512695312\n","SPS: 1300\n","global_step=36364, episodic_return=-72.20764923095703\n","global_step=36380, episodic_return=-291.29193115234375\n","global_step=36636, episodic_return=-252.2803192138672\n","global_step=36736, episodic_return=-127.81745910644531\n","global_step=36848, episodic_return=-356.2801513671875\n","SPS: 1302\n","global_step=36988, episodic_return=-73.114013671875\n","global_step=37032, episodic_return=-151.9540252685547\n","global_step=37248, episodic_return=-113.43653106689453\n","global_step=37284, episodic_return=-152.31192016601562\n","SPS: 1303\n","global_step=37608, episodic_return=-78.83438873291016\n","global_step=37616, episodic_return=-96.21194458007812\n","global_step=37760, episodic_return=-159.75982666015625\n","global_step=37768, episodic_return=-196.40980529785156\n","SPS: 1304\n","global_step=37992, episodic_return=-532.1104736328125\n","global_step=38140, episodic_return=-110.7060317993164\n","global_step=38344, episodic_return=-346.64276123046875\n","global_step=38372, episodic_return=-219.81915283203125\n","SPS: 1304\n","global_step=38548, episodic_return=-61.55450439453125\n","global_step=38676, episodic_return=-45.88613510131836\n","global_step=38796, episodic_return=-33.43865203857422\n","SPS: 1305\n","global_step=39020, episodic_return=-42.749351501464844\n","global_step=39044, episodic_return=-70.1358413696289\n","global_step=39200, episodic_return=-89.80352783203125\n","global_step=39408, episodic_return=-183.77685546875\n","SPS: 1306\n","global_step=39472, episodic_return=-105.62699127197266\n","global_step=39536, episodic_return=-70.7164077758789\n","global_step=39628, episodic_return=-204.6976318359375\n","global_step=39848, episodic_return=0.154754638671875\n","global_step=39924, episodic_return=-293.5493469238281\n","SPS: 1307\n","global_step=39976, episodic_return=-28.440330505371094\n","global_step=40056, episodic_return=-63.92041015625\n","global_step=40276, episodic_return=-17.661903381347656\n","SPS: 1308\n","global_step=40516, episodic_return=-139.6790008544922\n","global_step=40584, episodic_return=-78.0359115600586\n","global_step=40592, episodic_return=-80.26106262207031\n","global_step=40664, episodic_return=-122.14990234375\n","SPS: 1309\n","global_step=40972, episodic_return=-63.43648147583008\n","global_step=41068, episodic_return=-27.503204345703125\n","global_step=41076, episodic_return=-95.48998260498047\n","global_step=41252, episodic_return=-130.25328063964844\n","SPS: 1309\n","global_step=41688, episodic_return=-293.40521240234375\n","global_step=41760, episodic_return=-155.57455444335938\n","global_step=41776, episodic_return=-204.62969970703125\n","global_step=41920, episodic_return=-152.03244018554688\n","SPS: 1310\n","global_step=42216, episodic_return=-111.5422134399414\n","global_step=42380, episodic_return=-314.3004150390625\n","SPS: 1310\n","global_step=42536, episodic_return=-294.8297119140625\n","global_step=42624, episodic_return=-183.4130859375\n","global_step=42672, episodic_return=-117.1762924194336\n","global_step=42776, episodic_return=42.76127624511719\n","SPS: 1306\n","global_step=43188, episodic_return=-494.0180358886719\n","global_step=43220, episodic_return=-236.94618225097656\n","global_step=43328, episodic_return=-241.00003051757812\n","global_step=43404, episodic_return=-161.89154052734375\n","SPS: 1301\n","global_step=43556, episodic_return=-96.88160705566406\n","global_step=43616, episodic_return=-121.76553344726562\n","global_step=43796, episodic_return=-36.84417724609375\n","global_step=43864, episodic_return=5.650634765625\n","SPS: 1297\n","global_step=44184, episodic_return=-61.05548858642578\n","global_step=44304, episodic_return=-68.5385513305664\n","global_step=44312, episodic_return=-128.4666290283203\n","global_step=44380, episodic_return=-72.57341003417969\n","SPS: 1292\n","global_step=44652, episodic_return=-186.8479766845703\n","global_step=44704, episodic_return=-189.0137939453125\n","global_step=44768, episodic_return=-57.45933532714844\n","global_step=44892, episodic_return=-331.3389587402344\n","SPS: 1287\n","global_step=45172, episodic_return=-203.47470092773438\n","global_step=45292, episodic_return=-456.474365234375\n","global_step=45344, episodic_return=-3.9107894897460938\n","global_step=45428, episodic_return=-321.8004455566406\n","SPS: 1282\n","global_step=45728, episodic_return=-233.25669860839844\n","global_step=45852, episodic_return=-34.269805908203125\n","global_step=45904, episodic_return=-75.62015533447266\n","global_step=45928, episodic_return=-203.43817138671875\n","SPS: 1282\n","global_step=46272, episodic_return=-288.4901123046875\n","global_step=46364, episodic_return=-181.5059814453125\n","global_step=46556, episodic_return=-19.75408172607422\n","SPS: 1282\n","global_step=46692, episodic_return=-126.68495178222656\n","global_step=46800, episodic_return=-41.90133285522461\n","global_step=46828, episodic_return=-64.28236389160156\n","global_step=47012, episodic_return=-93.78804016113281\n","SPS: 1283\n","global_step=47212, episodic_return=-178.152587890625\n","global_step=47308, episodic_return=-150.48904418945312\n","global_step=47484, episodic_return=-67.35350799560547\n","global_step=47488, episodic_return=-60.3720703125\n","global_step=47544, episodic_return=-82.44261932373047\n","SPS: 1284\n","global_step=47892, episodic_return=-19.51984405517578\n","global_step=47996, episodic_return=-32.128204345703125\n","global_step=48032, episodic_return=-240.075439453125\n","SPS: 1284\n","global_step=48256, episodic_return=-196.95127868652344\n","SPS: 1285\n","global_step=48656, episodic_return=-89.96942138671875\n","global_step=48780, episodic_return=-48.70652389526367\n","global_step=48800, episodic_return=-89.39413452148438\n","global_step=48888, episodic_return=-102.01203155517578\n","global_step=49100, episodic_return=-139.66384887695312\n","SPS: 1285\n","global_step=49216, episodic_return=-159.6902618408203\n","global_step=49368, episodic_return=-135.92160034179688\n","global_step=49648, episodic_return=-11.353279113769531\n","SPS: 1286\n","global_step=49672, episodic_return=-119.63040161132812\n","global_step=49720, episodic_return=-19.827499389648438\n","global_step=50120, episodic_return=-95.7811050415039\n","SPS: 1286\n","global_step=50312, episodic_return=-225.63507080078125\n","global_step=50392, episodic_return=-179.87741088867188\n","global_step=50476, episodic_return=-250.1830291748047\n","global_step=50632, episodic_return=-75.92922973632812\n","SPS: 1286\n","global_step=50740, episodic_return=-188.1359405517578\n","global_step=50856, episodic_return=-189.2613067626953\n","global_step=51144, episodic_return=-221.67466735839844\n","global_step=51184, episodic_return=-155.54476928710938\n","SPS: 1286\n","global_step=51256, episodic_return=-43.68266677856445\n","global_step=51664, episodic_return=-114.92301940917969\n","SPS: 1282\n","global_step=51800, episodic_return=-230.57608032226562\n","global_step=51824, episodic_return=-108.78070068359375\n","global_step=52024, episodic_return=-63.09974670410156\n","SPS: 1282\n","global_step=52340, episodic_return=-135.94590759277344\n","global_step=52548, episodic_return=-28.14983367919922\n","global_step=52696, episodic_return=-128.88168334960938\n","SPS: 1282\n","global_step=52764, episodic_return=-39.688053131103516\n","global_step=52832, episodic_return=-137.65493774414062\n","global_step=53040, episodic_return=-179.08831787109375\n","SPS: 1283\n","global_step=53408, episodic_return=-25.281036376953125\n","global_step=53536, episodic_return=-177.2284393310547\n","global_step=53648, episodic_return=-114.10408020019531\n","SPS: 1282\n","global_step=53940, episodic_return=-364.89306640625\n","global_step=54116, episodic_return=-167.23171997070312\n","global_step=54168, episodic_return=-67.73815155029297\n","SPS: 1282\n","global_step=54460, episodic_return=-208.73507690429688\n","global_step=54596, episodic_return=-314.4515075683594\n","global_step=54768, episodic_return=-51.61624526977539\n","global_step=54784, episodic_return=-217.18309020996094\n","SPS: 1282\n","global_step=55152, episodic_return=-8.287918090820312\n","SPS: 1282\n","global_step=55340, episodic_return=-62.78839111328125\n","global_step=55412, episodic_return=-247.41990661621094\n","global_step=55484, episodic_return=-394.1741943359375\n","global_step=55760, episodic_return=-76.17310333251953\n","SPS: 1282\n","global_step=55812, episodic_return=-167.91207885742188\n","global_step=55964, episodic_return=-167.88919067382812\n","global_step=56196, episodic_return=-192.48947143554688\n","SPS: 1283\n","global_step=56380, episodic_return=-280.118896484375\n","SPS: 1282\n","global_step=57028, episodic_return=-321.7650146484375\n","global_step=57056, episodic_return=-255.61940002441406\n","global_step=57172, episodic_return=-27.759437561035156\n","SPS: 1279\n","global_step=57384, episodic_return=-244.68359375\n","global_step=57676, episodic_return=-207.11708068847656\n","global_step=57716, episodic_return=-232.87734985351562\n","global_step=57800, episodic_return=-207.27127075195312\n","SPS: 1279\n","global_step=58228, episodic_return=-318.66107177734375\n","SPS: 1277\n","global_step=58500, episodic_return=-215.58787536621094\n","global_step=58588, episodic_return=-323.0425720214844\n","global_step=58644, episodic_return=-450.7477111816406\n","global_step=58732, episodic_return=-142.62423706054688\n","SPS: 1272\n","global_step=59068, episodic_return=-38.35234832763672\n","SPS: 1268\n","global_step=59420, episodic_return=-268.41961669921875\n","global_step=59508, episodic_return=-115.40853881835938\n","global_step=59652, episodic_return=-235.08119201660156\n","global_step=59880, episodic_return=-232.62591552734375\n","SPS: 1264\n","global_step=60100, episodic_return=-143.24502563476562\n","global_step=60216, episodic_return=33.38905334472656\n","global_step=60392, episodic_return=-305.8302001953125\n","SPS: 1260\n","global_step=60564, episodic_return=-26.986549377441406\n","global_step=60580, episodic_return=-314.7624206542969\n","global_step=60804, episodic_return=-104.75623321533203\n","SPS: 1256\n","global_step=61220, episodic_return=-108.04527282714844\n","global_step=61268, episodic_return=-192.9451446533203\n","SPS: 1256\n","global_step=61760, episodic_return=-19.97838592529297\n","SPS: 1249\n","global_step=61980, episodic_return=-101.56338500976562\n","SPS: 1244\n","global_step=62800, episodic_return=-306.6361083984375\n","global_step=62884, episodic_return=-319.5085144042969\n","SPS: 1237\n","global_step=63444, episodic_return=-10.181282043457031\n","global_step=63476, episodic_return=-94.69485473632812\n","SPS: 1229\n","SPS: 1221\n","global_step=64068, episodic_return=-391.6173095703125\n","global_step=64076, episodic_return=-290.3075866699219\n","global_step=64392, episodic_return=29.99721336364746\n","SPS: 1213\n","global_step=64556, episodic_return=-102.12168884277344\n","global_step=64804, episodic_return=28.53169059753418\n","global_step=64832, episodic_return=9.665069580078125\n","SPS: 1207\n","global_step=65132, episodic_return=-253.4605712890625\n","global_step=65188, episodic_return=-21.43775177001953\n","global_step=65388, episodic_return=-44.354801177978516\n","global_step=65512, episodic_return=-43.77385330200195\n","SPS: 1208\n","global_step=65936, episodic_return=-150.1501007080078\n","SPS: 1204\n","global_step=66104, episodic_return=-278.2059326171875\n","global_step=66108, episodic_return=-416.3448791503906\n","global_step=66120, episodic_return=-237.75843811035156\n","SPS: 1203\n","global_step=66568, episodic_return=-251.26658630371094\n","global_step=66700, episodic_return=-282.908935546875\n","global_step=66732, episodic_return=-412.4810485839844\n","global_step=66824, episodic_return=-72.04473114013672\n","SPS: 1202\n","global_step=67184, episodic_return=-302.0157775878906\n","global_step=67208, episodic_return=-37.55923843383789\n","global_step=67340, episodic_return=-167.09555053710938\n","global_step=67520, episodic_return=-70.09903717041016\n","SPS: 1202\n","global_step=67696, episodic_return=-15.133270263671875\n","global_step=67884, episodic_return=-68.70944213867188\n","SPS: 1203\n","global_step=68180, episodic_return=-277.209716796875\n","SPS: 1203\n","global_step=68668, episodic_return=-48.72267532348633\n","global_step=68720, episodic_return=-53.76161193847656\n","global_step=68740, episodic_return=-216.10169982910156\n","global_step=69056, episodic_return=-93.13800811767578\n","SPS: 1202\n","global_step=69264, episodic_return=-306.2619934082031\n","global_step=69424, episodic_return=-235.0141143798828\n","SPS: 1200\n","global_step=69816, episodic_return=-25.048789978027344\n","global_step=69852, episodic_return=-211.23141479492188\n","global_step=69960, episodic_return=-587.164794921875\n","SPS: 1198\n","global_step=70516, episodic_return=-300.0638427734375\n","global_step=70556, episodic_return=-19.45679473876953\n","global_step=70616, episodic_return=-94.31026458740234\n","SPS: 1191\n","SPS: 1184\n","global_step=71184, episodic_return=-164.22169494628906\n","global_step=71244, episodic_return=-157.24295043945312\n","global_step=71252, episodic_return=-167.62652587890625\n","global_step=71340, episodic_return=21.358304977416992\n","global_step=71648, episodic_return=-83.99845886230469\n","SPS: 1180\n","global_step=71860, episodic_return=-225.04879760742188\n","global_step=71996, episodic_return=-61.28150939941406\n","global_step=72136, episodic_return=-66.74616241455078\n","SPS: 1177\n","global_step=72444, episodic_return=-100.546875\n","global_step=72664, episodic_return=-55.33062744140625\n","SPS: 1177\n","global_step=72756, episodic_return=-297.88409423828125\n","global_step=72888, episodic_return=-39.36730194091797\n","global_step=73052, episodic_return=15.320045471191406\n","SPS: 1178\n","global_step=73296, episodic_return=-223.69500732421875\n","global_step=73664, episodic_return=-248.74520874023438\n","SPS: 1178\n","global_step=73920, episodic_return=-78.02318572998047\n","global_step=73964, episodic_return=-52.79359436035156\n","global_step=74172, episodic_return=-194.89663696289062\n","SPS: 1177\n","global_step=74636, episodic_return=-94.00726318359375\n","global_step=74684, episodic_return=-226.26571655273438\n","SPS: 1175\n","global_step=74932, episodic_return=8.504364013671875\n","SPS: 1173\n","global_step=75272, episodic_return=-67.54695129394531\n","global_step=75328, episodic_return=-87.33248901367188\n","SPS: 1170\n","global_step=75828, episodic_return=-128.6304168701172\n","SPS: 1166\n","global_step=76320, episodic_return=-58.917537689208984\n","global_step=76536, episodic_return=-245.6614990234375\n","global_step=76556, episodic_return=-156.4291534423828\n","global_step=76756, episodic_return=100.39557647705078\n","SPS: 1163\n","global_step=76944, episodic_return=-9.515518188476562\n","global_step=77084, episodic_return=-324.3929138183594\n","global_step=77112, episodic_return=-26.30328369140625\n","SPS: 1164\n","global_step=77548, episodic_return=-273.9204406738281\n","global_step=77688, episodic_return=-217.5697784423828\n","SPS: 1164\n","global_step=77832, episodic_return=-287.7191162109375\n","global_step=77932, episodic_return=16.73473358154297\n","global_step=78096, episodic_return=-285.3334655761719\n","SPS: 1165\n","global_step=78408, episodic_return=-320.0462646484375\n","global_step=78488, episodic_return=-54.468475341796875\n","global_step=78788, episodic_return=24.055557250976562\n","SPS: 1165\n","global_step=79208, episodic_return=-29.73371124267578\n","SPS: 1164\n","global_step=79388, episodic_return=11.9949951171875\n","global_step=79576, episodic_return=-101.91698455810547\n","SPS: 1163\n","global_step=80128, episodic_return=-134.94091796875\n","global_step=80184, episodic_return=-162.36990356445312\n","SPS: 1162\n","global_step=80624, episodic_return=-11.789810180664062\n","SPS: 1160\n","global_step=80916, episodic_return=-293.78582763671875\n","global_step=81024, episodic_return=-205.75900268554688\n","SPS: 1157\n","global_step=81560, episodic_return=-162.1476287841797\n","global_step=81648, episodic_return=-214.35577392578125\n","global_step=81756, episodic_return=-267.92437744140625\n","global_step=81832, episodic_return=39.20896911621094\n","SPS: 1156\n","global_step=82392, episodic_return=-74.98377990722656\n","SPS: 1156\n","global_step=82472, episodic_return=-11.929603576660156\n","global_step=82740, episodic_return=-68.25595092773438\n","SPS: 1153\n","global_step=83028, episodic_return=-12.985496520996094\n","global_step=83424, episodic_return=-95.62630462646484\n","global_step=83448, episodic_return=-96.79945373535156\n","SPS: 1148\n","global_step=83624, episodic_return=-204.30029296875\n","SPS: 1145\n","global_step=84252, episodic_return=35.3553466796875\n","SPS: 1139\n","global_step=84628, episodic_return=-240.3260498046875\n","global_step=84636, episodic_return=-314.56005859375\n","SPS: 1135\n","global_step=85004, episodic_return=-44.151058197021484\n","global_step=85088, episodic_return=-37.88206100463867\n","SPS: 1132\n","global_step=85648, episodic_return=88.90123748779297\n","global_step=85888, episodic_return=-240.54666137695312\n","SPS: 1131\n","global_step=86028, episodic_return=-236.882568359375\n","SPS: 1130\n","global_step=86556, episodic_return=-87.44078063964844\n","global_step=86656, episodic_return=-233.1773681640625\n","SPS: 1128\n","global_step=87416, episodic_return=-89.43818664550781\n","global_step=87524, episodic_return=-283.30169677734375\n","SPS: 1124\n","SPS: 1120\n","global_step=88220, episodic_return=-211.4241943359375\n","global_step=88252, episodic_return=-40.38379669189453\n","SPS: 1115\n","global_step=88636, episodic_return=32.0540771484375\n","global_step=88804, episodic_return=-15.99737548828125\n","SPS: 1112\n","global_step=89152, episodic_return=1.1556625366210938\n","global_step=89532, episodic_return=-172.1832275390625\n","SPS: 1110\n","global_step=89640, episodic_return=0.44052886962890625\n","global_step=89888, episodic_return=107.16084289550781\n","global_step=89936, episodic_return=-15.654205322265625\n","SPS: 1108\n","global_step=90168, episodic_return=-67.76009368896484\n","SPS: 1107\n","global_step=90676, episodic_return=48.17552185058594\n","global_step=90860, episodic_return=-91.77411651611328\n","global_step=91092, episodic_return=12.45867919921875\n","SPS: 1106\n","global_step=91328, episodic_return=-235.98019409179688\n","SPS: 1105\n","global_step=91676, episodic_return=-42.30315399169922\n","global_step=91860, episodic_return=-42.93678283691406\n","SPS: 1103\n","global_step=92220, episodic_return=118.81687927246094\n","global_step=92292, episodic_return=-183.4189910888672\n","global_step=92352, episodic_return=-13.821998596191406\n","global_step=92412, episodic_return=-75.11962890625\n","SPS: 1104\n","global_step=92804, episodic_return=4.471153259277344\n","global_step=92948, episodic_return=-262.46087646484375\n","global_step=93024, episodic_return=-203.10546875\n","SPS: 1105\n","global_step=93408, episodic_return=-105.13594055175781\n","global_step=93600, episodic_return=8.548683166503906\n","global_step=93620, episodic_return=-230.49951171875\n","SPS: 1104\n","global_step=93716, episodic_return=-157.38674926757812\n","global_step=94056, episodic_return=37.393768310546875\n","SPS: 1103\n","global_step=94536, episodic_return=-14.426910400390625\n","global_step=94688, episodic_return=-0.05837249755859375\n","SPS: 1100\n","global_step=94800, episodic_return=-92.21581268310547\n","global_step=94960, episodic_return=-152.80001831054688\n","SPS: 1098\n","global_step=95480, episodic_return=-233.2613067626953\n","global_step=95668, episodic_return=-158.9064178466797\n","SPS: 1095\n","global_step=95780, episodic_return=-21.762725830078125\n","global_step=96072, episodic_return=-122.25009155273438\n","SPS: 1095\n","global_step=96468, episodic_return=-196.61959838867188\n","global_step=96492, episodic_return=-27.4486083984375\n","global_step=96560, episodic_return=-12.15704345703125\n","SPS: 1095\n","global_step=97132, episodic_return=-57.89292526245117\n","global_step=97208, episodic_return=-47.80504608154297\n","SPS: 1095\n","global_step=97500, episodic_return=-52.793968200683594\n","global_step=97720, episodic_return=-55.65544509887695\n","SPS: 1095\n","global_step=97996, episodic_return=-11.4501953125\n","SPS: 1094\n","global_step=98580, episodic_return=12.364852905273438\n","global_step=98584, episodic_return=38.271209716796875\n","global_step=98592, episodic_return=-41.54656982421875\n","SPS: 1093\n","global_step=99028, episodic_return=-17.942764282226562\n","SPS: 1092\n","global_step=99480, episodic_return=117.91949462890625\n","SPS: 1091\n","global_step=99896, episodic_return=-225.3283233642578\n","global_step=99980, episodic_return=-41.15080261230469\n","SPS: 1090\n","global_step=100480, episodic_return=-5.267852783203125\n","global_step=100560, episodic_return=-8.512939453125\n","global_step=100776, episodic_return=18.692031860351562\n","SPS: 1090\n","SPS: 1089\n","global_step=101564, episodic_return=-113.81390380859375\n","global_step=101628, episodic_return=-18.350509643554688\n","SPS: 1087\n","global_step=102380, episodic_return=-96.5154800415039\n","SPS: 1085\n","global_step=102580, episodic_return=51.71733856201172\n","SPS: 1083\n","global_step=103120, episodic_return=-193.7357177734375\n","global_step=103364, episodic_return=-83.65869140625\n","SPS: 1081\n","global_step=103812, episodic_return=-7.06475830078125\n","SPS: 1080\n","global_step=104068, episodic_return=-28.706344604492188\n","SPS: 1078\n","global_step=104560, episodic_return=33.97677230834961\n","global_step=104620, episodic_return=-66.11923217773438\n","SPS: 1077\n","global_step=105076, episodic_return=-65.09541320800781\n","global_step=105356, episodic_return=-57.48299789428711\n","global_step=105400, episodic_return=-47.5284423828125\n","SPS: 1075\n","global_step=105596, episodic_return=3.0410690307617188\n","SPS: 1072\n","global_step=106092, episodic_return=-11.0869140625\n","global_step=106156, episodic_return=-30.216026306152344\n","global_step=106236, episodic_return=72.98687744140625\n","SPS: 1069\n","global_step=106580, episodic_return=133.11781311035156\n","global_step=106652, episodic_return=-43.94453430175781\n","global_step=106776, episodic_return=-76.00859832763672\n","global_step=106860, episodic_return=19.32817840576172\n","SPS: 1067\n","global_step=107116, episodic_return=21.2008056640625\n","global_step=107508, episodic_return=-82.07688903808594\n","SPS: 1065\n","global_step=107568, episodic_return=-46.048187255859375\n","global_step=107844, episodic_return=-169.25897216796875\n","SPS: 1065\n","global_step=108180, episodic_return=31.107147216796875\n","global_step=108468, episodic_return=32.56681823730469\n","global_step=108484, episodic_return=-11.911216735839844\n","SPS: 1066\n","global_step=109028, episodic_return=-116.96485900878906\n","SPS: 1066\n","SPS: 1065\n","global_step=109584, episodic_return=-55.07638931274414\n","global_step=109772, episodic_return=-182.34439086914062\n","SPS: 1064\n","global_step=110276, episodic_return=-29.012588500976562\n","SPS: 1063\n","global_step=110860, episodic_return=101.83867645263672\n","global_step=111060, episodic_return=-42.77177810668945\n","SPS: 1061\n","global_step=111304, episodic_return=-83.60090637207031\n","global_step=111580, episodic_return=-20.831626892089844\n","SPS: 1061\n","global_step=112120, episodic_return=-11.967269897460938\n","SPS: 1060\n","global_step=112272, episodic_return=-69.11656951904297\n","SPS: 1058\n","global_step=112692, episodic_return=-58.705596923828125\n","global_step=113028, episodic_return=119.58540344238281\n","SPS: 1057\n","global_step=113228, episodic_return=-36.13462448120117\n","global_step=113244, episodic_return=13.116165161132812\n","global_step=113464, episodic_return=-69.73014831542969\n","SPS: 1057\n","SPS: 1058\n","global_step=114384, episodic_return=-46.328887939453125\n","SPS: 1056\n","global_step=114744, episodic_return=-203.59915161132812\n","global_step=115140, episodic_return=-109.67620849609375\n","SPS: 1056\n","global_step=115364, episodic_return=-8.220016479492188\n","global_step=115368, episodic_return=-160.10638427734375\n","SPS: 1055\n","global_step=116092, episodic_return=-200.9075469970703\n","SPS: 1054\n","global_step=116452, episodic_return=-27.18048095703125\n","SPS: 1051\n","global_step=117204, episodic_return=-8.765785217285156\n","global_step=117244, episodic_return=111.5206527709961\n","SPS: 1046\n","SPS: 1043\n","global_step=117924, episodic_return=-67.10993194580078\n","global_step=118096, episodic_return=33.46855163574219\n","SPS: 1038\n","SPS: 1035\n","global_step=119100, episodic_return=-19.93425750732422\n","SPS: 1032\n","global_step=119368, episodic_return=84.22079467773438\n","global_step=119676, episodic_return=-86.72979736328125\n","SPS: 1030\n","global_step=119936, episodic_return=-42.0564079284668\n","global_step=120092, episodic_return=91.91890716552734\n","SPS: 1030\n","global_step=120784, episodic_return=-87.13140869140625\n","SPS: 1029\n","global_step=121096, episodic_return=-82.30023956298828\n","global_step=121252, episodic_return=-44.45478820800781\n","SPS: 1029\n","SPS: 1028\n","global_step=121980, episodic_return=-32.16485595703125\n","global_step=122232, episodic_return=-401.15032958984375\n","global_step=122276, episodic_return=11.95556640625\n","global_step=122368, episodic_return=19.39862823486328\n","SPS: 1027\n","SPS: 1028\n","global_step=122948, episodic_return=-169.31332397460938\n","global_step=123336, episodic_return=-69.78633117675781\n","SPS: 1028\n","SPS: 1027\n","global_step=123976, episodic_return=-176.55804443359375\n","global_step=124204, episodic_return=-71.86227416992188\n","global_step=124216, episodic_return=-198.29400634765625\n","SPS: 1026\n","global_step=124832, episodic_return=-21.63543701171875\n","SPS: 1026\n","global_step=125128, episodic_return=-22.47248077392578\n","SPS: 1025\n","global_step=125840, episodic_return=-13.327354431152344\n","SPS: 1024\n","global_step=125956, episodic_return=-194.42417907714844\n","global_step=126276, episodic_return=50.334938049316406\n","SPS: 1023\n","global_step=126776, episodic_return=19.796119689941406\n","SPS: 1023\n","global_step=127220, episodic_return=-103.16404724121094\n","global_step=127432, episodic_return=39.69236755371094\n","SPS: 1021\n","global_step=127920, episodic_return=-61.84416961669922\n","SPS: 1019\n","global_step=128204, episodic_return=125.5879135131836\n","SPS: 1016\n","global_step=128968, episodic_return=19.34283447265625\n","global_step=129004, episodic_return=-1.9879379272460938\n","SPS: 1014\n","SPS: 1013\n","global_step=129724, episodic_return=-118.59768676757812\n","global_step=129840, episodic_return=160.54586791992188\n","SPS: 1012\n","global_step=130060, episodic_return=24.60077667236328\n","SPS: 1010\n","global_step=131040, episodic_return=-42.23439407348633\n","SPS: 1009\n","global_step=131208, episodic_return=-203.37619018554688\n","global_step=131432, episodic_return=88.84659576416016\n","SPS: 1008\n","global_step=131784, episodic_return=6.3243255615234375\n","SPS: 1008\n","global_step=132256, episodic_return=-15.150245666503906\n","global_step=132464, episodic_return=-53.6329460144043\n","SPS: 1008\n","SPS: 1007\n","global_step=133168, episodic_return=-4.2969818115234375\n","global_step=133212, episodic_return=-68.1356201171875\n","SPS: 1006\n","global_step=133844, episodic_return=-10.568862915039062\n","global_step=133936, episodic_return=69.28691101074219\n","global_step=134060, episodic_return=157.5008544921875\n","SPS: 1006\n","global_step=134492, episodic_return=-37.11631774902344\n","SPS: 1006\n","global_step=134808, episodic_return=13.291603088378906\n","global_step=135040, episodic_return=106.68968200683594\n","SPS: 1005\n","global_step=135620, episodic_return=6.424461364746094\n","SPS: 1005\n","global_step=135984, episodic_return=-58.97566223144531\n","global_step=136020, episodic_return=-17.71685791015625\n","SPS: 1005\n","global_step=136328, episodic_return=-25.35095977783203\n","SPS: 1005\n","global_step=136848, episodic_return=-28.265357971191406\n","global_step=136984, episodic_return=-153.02072143554688\n","SPS: 1005\n","global_step=137620, episodic_return=-49.28644561767578\n","global_step=137628, episodic_return=-66.40394592285156\n","SPS: 1004\n","global_step=137936, episodic_return=44.480995178222656\n","SPS: 1002\n","SPS: 1000\n","global_step=139184, episodic_return=-209.30081176757812\n","SPS: 996\n","SPS: 994\n","global_step=139984, episodic_return=148.86453247070312\n","global_step=140184, episodic_return=-74.37382507324219\n","SPS: 992\n","global_step=140736, episodic_return=-21.194229125976562\n","SPS: 991\n","global_step=141280, episodic_return=-28.75989532470703\n","SPS: 990\n","global_step=141628, episodic_return=66.98621368408203\n","SPS: 989\n","global_step=141936, episodic_return=-10.728068351745605\n","SPS: 989\n","global_step=142604, episodic_return=69.62242126464844\n","SPS: 988\n","global_step=142968, episodic_return=-78.56869506835938\n","global_step=143344, episodic_return=7.2137298583984375\n","SPS: 987\n","SPS: 986\n","global_step=144112, episodic_return=-75.46470642089844\n","SPS: 985\n","global_step=144736, episodic_return=98.1012191772461\n","SPS: 983\n","global_step=145232, episodic_return=-175.8162841796875\n","global_step=145280, episodic_return=70.35674285888672\n","SPS: 982\n","global_step=145844, episodic_return=-211.78887939453125\n","SPS: 981\n","SPS: 980\n","global_step=146932, episodic_return=-7.387176513671875\n","SPS: 977\n","global_step=146968, episodic_return=21.958192825317383\n","SPS: 974\n","global_step=147932, episodic_return=-61.27389907836914\n","SPS: 969\n","global_step=147992, episodic_return=9.035087585449219\n","SPS: 966\n","global_step=148632, episodic_return=4.930046081542969\n","global_step=148812, episodic_return=7.9903564453125\n","SPS: 963\n","global_step=149232, episodic_return=31.224300384521484\n","global_step=149280, episodic_return=127.0267562866211\n","SPS: 962\n","global_step=149684, episodic_return=-8.011299133300781\n","global_step=149952, episodic_return=15.006927490234375\n","SPS: 963\n","SPS: 963\n","SPS: 962\n","SPS: 960\n","SPS: 956\n","SPS: 952\n","SPS: 949\n","global_step=153232, episodic_return=121.15258026123047\n","SPS: 945\n","global_step=153684, episodic_return=102.98309326171875\n","global_step=153952, episodic_return=62.623016357421875\n","SPS: 945\n","SPS: 944\n","SPS: 942\n","global_step=155464, episodic_return=-105.38313293457031\n","SPS: 938\n","global_step=156132, episodic_return=-16.39977264404297\n","SPS: 936\n","SPS: 935\n","global_step=156752, episodic_return=15.230506896972656\n","SPS: 933\n","global_step=157232, episodic_return=39.716285705566406\n","SPS: 932\n","global_step=157952, episodic_return=45.04071807861328\n","global_step=158068, episodic_return=-1.3642349243164062\n","global_step=158076, episodic_return=-67.26219177246094\n","SPS: 931\n","SPS: 931\n","global_step=158764, episodic_return=-112.52069854736328\n","global_step=159196, episodic_return=16.60350799560547\n","SPS: 931\n","SPS: 931\n","global_step=160056, episodic_return=-51.609039306640625\n","SPS: 930\n","global_step=160700, episodic_return=-90.18193817138672\n","SPS: 929\n","global_step=160944, episodic_return=-115.33163452148438\n","SPS: 929\n","global_step=161672, episodic_return=-2.6914138793945312\n","SPS: 928\n","global_step=162176, episodic_return=-36.29692077636719\n","SPS: 928\n","global_step=162764, episodic_return=131.03054809570312\n","SPS: 928\n","SPS: 928\n","SPS: 926\n","SPS: 923\n","global_step=164628, episodic_return=-152.36404418945312\n","SPS: 920\n","global_step=164944, episodic_return=177.43841552734375\n","SPS: 919\n","SPS: 918\n","global_step=166176, episodic_return=192.91400146484375\n","SPS: 917\n","global_step=166764, episodic_return=-9.119427680969238\n","SPS: 916\n","global_step=167020, episodic_return=-2.2775115966796875\n","SPS: 916\n","SPS: 915\n","SPS: 914\n","global_step=168628, episodic_return=146.95510864257812\n","global_step=168732, episodic_return=-19.557945251464844\n","global_step=168944, episodic_return=110.93785858154297\n","SPS: 913\n","SPS: 912\n","global_step=169544, episodic_return=-45.822147369384766\n","SPS: 911\n","SPS: 908\n","global_step=170764, episodic_return=105.5678939819336\n","SPS: 907\n","global_step=171080, episodic_return=-55.599884033203125\n","SPS: 905\n","SPS: 902\n","SPS: 898\n","global_step=172628, episodic_return=119.10865020751953\n","global_step=172944, episodic_return=57.157474517822266\n","SPS: 897\n","SPS: 896\n","SPS: 895\n","SPS: 894\n","global_step=174764, episodic_return=-50.82693862915039\n","global_step=175080, episodic_return=88.0936279296875\n","SPS: 893\n","SPS: 892\n","global_step=176072, episodic_return=-38.79845428466797\n","SPS: 892\n","global_step=176628, episodic_return=155.33746337890625\n","SPS: 891\n","global_step=176944, episodic_return=-138.63987731933594\n","SPS: 891\n","SPS: 890\n","SPS: 889\n","SPS: 887\n","global_step=178764, episodic_return=-82.51841735839844\n","SPS: 883\n","SPS: 878\n","global_step=180072, episodic_return=34.23906707763672\n","SPS: 874\n","global_step=180628, episodic_return=91.40962219238281\n","SPS: 872\n","global_step=180944, episodic_return=-9.235045433044434\n","SPS: 872\n","SPS: 871\n","global_step=181772, episodic_return=-58.12843322753906\n","SPS: 871\n","global_step=182764, episodic_return=101.66508483886719\n","SPS: 870\n","SPS: 870\n","global_step=183552, episodic_return=-42.05767059326172\n","SPS: 869\n","SPS: 867\n","global_step=184628, episodic_return=62.86309051513672\n","SPS: 866\n","global_step=184944, episodic_return=87.08799743652344\n","SPS: 865\n","global_step=185824, episodic_return=19.194839477539062\n","SPS: 865\n","SPS: 863\n","global_step=186764, episodic_return=-18.958335876464844\n","SPS: 861\n","SPS: 859\n","global_step=187552, episodic_return=-25.606203079223633\n","SPS: 858\n","SPS: 857\n","SPS: 854\n","global_step=188944, episodic_return=-40.20729446411133\n","SPS: 852\n","global_step=189824, episodic_return=78.65351104736328\n","SPS: 850\n","SPS: 849\n","global_step=190764, episodic_return=-30.484752655029297\n","SPS: 847\n","SPS: 844\n","global_step=191552, episodic_return=-40.59446716308594\n","SPS: 842\n","SPS: 839\n","global_step=192944, episodic_return=-38.86568069458008\n","SPS: 835\n","SPS: 833\n","global_step=193824, episodic_return=47.22949981689453\n","SPS: 831\n","SPS: 829\n","global_step=194764, episodic_return=5.226692199707031\n","SPS: 828\n","global_step=195552, episodic_return=-85.90470886230469\n","SPS: 827\n","SPS: 827\n","SPS: 826\n","global_step=196944, episodic_return=-21.555694580078125\n","SPS: 825\n","SPS: 824\n","global_step=197824, episodic_return=9.904471397399902\n","SPS: 822\n","SPS: 821\n","global_step=198764, episodic_return=-51.239688873291016\n","SPS: 819\n","global_step=199552, episodic_return=4.406955718994141\n","SPS: 817\n","SPS: 816\n","SPS: 815\n","global_step=200944, episodic_return=55.763179779052734\n","SPS: 814\n","SPS: 814\n","global_step=201824, episodic_return=96.36945343017578\n","SPS: 813\n","SPS: 813\n","global_step=202764, episodic_return=90.82230377197266\n","SPS: 812\n","global_step=203552, episodic_return=85.29704284667969\n","SPS: 811\n","SPS: 811\n","global_step=204684, episodic_return=-129.59649658203125\n","SPS: 809\n","SPS: 808\n","global_step=205824, episodic_return=-62.02021789550781\n","SPS: 806\n","SPS: 803\n","global_step=206764, episodic_return=-19.28391456604004\n","SPS: 802\n","SPS: 801\n","global_step=207552, episodic_return=97.18773651123047\n","SPS: 801\n","SPS: 800\n","global_step=208684, episodic_return=26.632383346557617\n","SPS: 800\n","SPS: 799\n","global_step=209824, episodic_return=-30.90487289428711\n","SPS: 798\n","SPS: 797\n","global_step=210764, episodic_return=-78.4405517578125\n","SPS: 795\n","SPS: 793\n","global_step=211552, episodic_return=-16.88614273071289\n","SPS: 791\n","SPS: 789\n","global_step=212684, episodic_return=57.19608688354492\n","SPS: 787\n","SPS: 785\n","global_step=213824, episodic_return=-67.30419921875\n","SPS: 784\n","SPS: 782\n","global_step=214764, episodic_return=-94.81290435791016\n","SPS: 781\n","global_step=215552, episodic_return=-100.49890899658203\n","SPS: 780\n","SPS: 779\n","SPS: 778\n","global_step=216684, episodic_return=12.237842559814453\n","SPS: 777\n","SPS: 775\n","global_step=217824, episodic_return=-83.63232421875\n","SPS: 772\n","SPS: 771\n","global_step=218764, episodic_return=-118.82295227050781\n","SPS: 770\n","global_step=219552, episodic_return=-66.37650299072266\n","SPS: 769\n","SPS: 769\n","SPS: 769\n","global_step=220684, episodic_return=43.18894577026367\n","SPS: 769\n","SPS: 768\n","global_step=221824, episodic_return=-76.72341918945312\n","SPS: 768\n","SPS: 767\n","global_step=222764, episodic_return=-12.936349868774414\n","SPS: 767\n","global_step=223552, episodic_return=-3.1699650287628174\n","SPS: 766\n","SPS: 765\n","global_step=224684, episodic_return=-24.201723098754883\n","SPS: 763\n","SPS: 762\n","SPS: 760\n","global_step=225824, episodic_return=-18.871484756469727\n","SPS: 759\n","global_step=226764, episodic_return=-44.62932586669922\n","SPS: 758\n","SPS: 758\n","global_step=227552, episodic_return=-23.216196060180664\n","SPS: 757\n","SPS: 756\n","global_step=228684, episodic_return=-30.99916648864746\n","SPS: 755\n","SPS: 754\n","global_step=229824, episodic_return=-37.702762603759766\n","SPS: 752\n","SPS: 751\n","global_step=230764, episodic_return=-73.2296142578125\n","SPS: 749\n","SPS: 748\n","global_step=231552, episodic_return=-94.76612854003906\n","SPS: 747\n","SPS: 746\n","global_step=232684, episodic_return=-65.02286529541016\n","SPS: 746\n","SPS: 745\n","global_step=233824, episodic_return=-92.20640563964844\n","SPS: 744\n","SPS: 744\n","global_step=234764, episodic_return=-87.0000228881836\n","SPS: 743\n","SPS: 743\n","global_step=235552, episodic_return=-38.225364685058594\n","SPS: 743\n","SPS: 743\n","global_step=236684, episodic_return=-50.72796630859375\n","SPS: 742\n","global_step=237268, episodic_return=-115.03956604003906\n","SPS: 742\n","SPS: 741\n","SPS: 739\n","global_step=238764, episodic_return=-13.08251953125\n","SPS: 739\n","global_step=239552, episodic_return=-2.7748920917510986\n","SPS: 738\n","SPS: 738\n","SPS: 737\n","global_step=240684, episodic_return=-28.374637603759766\n","SPS: 737\n","global_step=241268, episodic_return=6.759311199188232\n","SPS: 737\n","SPS: 736\n","SPS: 735\n","global_step=242764, episodic_return=-77.66874694824219\n","SPS: 734\n","global_step=243552, episodic_return=-41.50791931152344\n","SPS: 733\n","SPS: 732\n","global_step=244684, episodic_return=-26.221811294555664\n","SPS: 730\n","SPS: 729\n","global_step=245268, episodic_return=-59.67810821533203\n","SPS: 729\n","SPS: 728\n","global_step=246764, episodic_return=-24.350934982299805\n","SPS: 727\n","SPS: 727\n","global_step=247552, episodic_return=-44.4078254699707\n","SPS: 727\n","SPS: 727\n","global_step=248684, episodic_return=-19.690553665161133\n","SPS: 727\n","global_step=248864, episodic_return=-132.52459716796875\n","SPS: 727\n","SPS: 727\n","SPS: 726\n","global_step=250764, episodic_return=-0.3085368871688843\n","SPS: 725\n","SPS: 723\n","global_step=251552, episodic_return=-43.75285720825195\n","SPS: 722\n","SPS: 721\n","global_step=252684, episodic_return=-11.912492752075195\n","global_step=252864, episodic_return=-24.97067642211914\n","SPS: 721\n","SPS: 721\n","SPS: 721\n","SPS: 720\n","global_step=254764, episodic_return=-37.03524398803711\n","SPS: 719\n","SPS: 718\n","global_step=255552, episodic_return=-59.19118118286133\n","SPS: 718\n","SPS: 716\n","global_step=256684, episodic_return=-42.54875564575195\n","global_step=256864, episodic_return=-4.461368560791016\n","SPS: 716\n","SPS: 715\n","SPS: 714\n","SPS: 713\n","global_step=258764, episodic_return=18.94056510925293\n","SPS: 712\n","global_step=259552, episodic_return=11.240188598632812\n","SPS: 711\n","SPS: 711\n","SPS: 711\n","global_step=260684, episodic_return=33.986759185791016\n","global_step=260864, episodic_return=62.85828399658203\n","SPS: 710\n","SPS: 710\n","SPS: 709\n","SPS: 707\n","global_step=262764, episodic_return=-13.256265640258789\n","SPS: 705\n","global_step=263552, episodic_return=33.52314758300781\n","SPS: 703\n","SPS: 703\n","global_step=264684, episodic_return=-10.623781204223633\n","SPS: 701\n","global_step=264864, episodic_return=-21.03203582763672\n","SPS: 701\n","SPS: 701\n","SPS: 701\n","SPS: 700\n","global_step=266764, episodic_return=3.087031364440918\n","SPS: 699\n","global_step=267552, episodic_return=-19.42699432373047\n","SPS: 698\n","SPS: 698\n","global_step=268684, episodic_return=17.638010025024414\n","SPS: 696\n","global_step=268864, episodic_return=85.50932312011719\n","SPS: 695\n","SPS: 694\n","SPS: 693\n","global_step=270764, episodic_return=18.034059524536133\n","SPS: 692\n","SPS: 691\n","global_step=271552, episodic_return=-35.604759216308594\n","SPS: 691\n","SPS: 691\n","global_step=272684, episodic_return=-71.97447967529297\n","global_step=272864, episodic_return=-19.788747787475586\n","SPS: 690\n","SPS: 690\n","SPS: 690\n","SPS: 689\n","global_step=274764, episodic_return=-23.146364212036133\n","SPS: 687\n","SPS: 686\n","global_step=275552, episodic_return=2.4747507572174072\n","SPS: 686\n","SPS: 685\n","global_step=276684, episodic_return=1.6659995317459106\n","global_step=276864, episodic_return=14.89195442199707\n","SPS: 685\n","SPS: 685\n","SPS: 685\n","global_step=278056, episodic_return=-99.67320251464844\n","SPS: 685\n","SPS: 685\n","global_step=279552, episodic_return=-8.393333435058594\n","SPS: 684\n","global_step=280024, episodic_return=-171.9132080078125\n","SPS: 684\n","SPS: 683\n","global_step=280864, episodic_return=-58.56867218017578\n","SPS: 683\n","SPS: 683\n","global_step=282056, episodic_return=-34.963661193847656\n","SPS: 682\n","SPS: 682\n","SPS: 681\n","global_step=283552, episodic_return=-30.56487464904785\n","SPS: 681\n","global_step=284024, episodic_return=9.831066131591797\n","SPS: 680\n","SPS: 680\n","global_step=284864, episodic_return=-8.758699417114258\n","SPS: 680\n","global_step=285492, episodic_return=-78.80670166015625\n","SPS: 680\n","SPS: 680\n","SPS: 680\n","SPS: 679\n","global_step=287552, episodic_return=23.078554153442383\n","SPS: 677\n","global_step=288024, episodic_return=58.84067153930664\n","SPS: 676\n","SPS: 676\n","global_step=288864, episodic_return=-0.21897399425506592\n","SPS: 675\n","global_step=289492, episodic_return=29.522825241088867\n","SPS: 675\n","SPS: 675\n","SPS: 675\n","SPS: 674\n","global_step=291552, episodic_return=57.18810272216797\n","SPS: 673\n","global_step=292024, episodic_return=19.25048065185547\n","SPS: 673\n","global_step=292864, episodic_return=22.621843338012695\n","SPS: 673\n","SPS: 673\n","global_step=293492, episodic_return=-10.31857681274414\n","SPS: 673\n","SPS: 672\n","global_step=294840, episodic_return=-150.3659210205078\n","SPS: 671\n","global_step=295128, episodic_return=-128.29812622070312\n","SPS: 671\n","SPS: 671\n","global_step=296300, episodic_return=-179.93521118164062\n","SPS: 670\n","SPS: 670\n","SPS: 669\n","global_step=297492, episodic_return=-22.391080856323242\n","SPS: 669\n","SPS: 669\n","global_step=298840, episodic_return=-15.15560531616211\n","SPS: 669\n","global_step=299128, episodic_return=-48.370269775390625\n","SPS: 669\n","SPS: 669\n","global_step=300300, episodic_return=60.91995620727539\n","SPS: 669\n","SPS: 669\n","global_step=301492, episodic_return=42.35478591918945\n","SPS: 668\n","global_step=301984, episodic_return=-122.01022338867188\n","SPS: 668\n","SPS: 667\n","global_step=302840, episodic_return=6.09720516204834\n","SPS: 667\n","global_step=303188, episodic_return=-129.26959228515625\n","SPS: 668\n","SPS: 667\n","SPS: 667\n","SPS: 666\n","global_step=305492, episodic_return=-25.8351993560791\n","SPS: 665\n","global_step=305984, episodic_return=71.67581176757812\n","SPS: 664\n","global_step=306348, episodic_return=56.49143981933594\n","SPS: 664\n","global_step=307188, episodic_return=20.707448959350586\n","SPS: 664\n","SPS: 664\n","SPS: 663\n","global_step=308436, episodic_return=-111.1820068359375\n","SPS: 663\n","SPS: 662\n","global_step=309492, episodic_return=82.91944122314453\n","SPS: 662\n","global_step=309992, episodic_return=-156.60128784179688\n","SPS: 662\n","global_step=310348, episodic_return=20.331615447998047\n","SPS: 662\n","SPS: 662\n","SPS: 662\n","global_step=311848, episodic_return=-171.038330078125\n","SPS: 662\n","global_step=312512, episodic_return=-133.80247497558594\n","SPS: 661\n","global_step=313232, episodic_return=-160.68502807617188\n","SPS: 661\n","global_step=313660, episodic_return=40.33268737792969\n","SPS: 661\n","global_step=314328, episodic_return=145.77603149414062\n","SPS: 662\n","SPS: 662\n","SPS: 661\n","SPS: 660\n","SPS: 658\n","global_step=316512, episodic_return=60.670326232910156\n","global_step=316572, episodic_return=-74.5474853515625\n","SPS: 658\n","global_step=317232, episodic_return=46.833621978759766\n","SPS: 658\n","global_step=317660, episodic_return=70.37979888916016\n","SPS: 658\n","SPS: 658\n","global_step=318524, episodic_return=-73.31932067871094\n","SPS: 658\n","SPS: 658\n","SPS: 657\n","global_step=320016, episodic_return=-147.20001220703125\n","global_step=320512, episodic_return=71.0091781616211\n","SPS: 656\n","SPS: 656\n","global_step=321232, episodic_return=72.20604705810547\n","SPS: 656\n","SPS: 655\n","global_step=322524, episodic_return=28.774696350097656\n","SPS: 655\n","SPS: 655\n","SPS: 655\n","global_step=324016, episodic_return=-16.68436622619629\n","SPS: 654\n","global_step=324512, episodic_return=5.297444820404053\n","SPS: 654\n","SPS: 654\n","global_step=325232, episodic_return=83.4283447265625\n","SPS: 654\n","SPS: 654\n","global_step=326524, episodic_return=85.73592376708984\n","SPS: 654\n","SPS: 654\n","SPS: 654\n","global_step=328016, episodic_return=-6.511110305786133\n","global_step=328080, episodic_return=-119.72945404052734\n","SPS: 654\n","SPS: 654\n","SPS: 654\n","global_step=329232, episodic_return=8.156699180603027\n","SPS: 653\n","SPS: 653\n","global_step=330524, episodic_return=23.295547485351562\n","SPS: 652\n","SPS: 652\n","SPS: 651\n","global_step=331944, episodic_return=101.21075439453125\n","global_step=332016, episodic_return=15.367134094238281\n","SPS: 651\n","SPS: 651\n","global_step=333232, episodic_return=33.72958755493164\n","SPS: 651\n","SPS: 651\n","SPS: 651\n","global_step=334524, episodic_return=80.25595092773438\n","SPS: 650\n","SPS: 650\n","SPS: 649\n","global_step=335944, episodic_return=-4.614491939544678\n","global_step=336016, episodic_return=-32.833045959472656\n","SPS: 649\n","SPS: 648\n","global_step=337232, episodic_return=71.52897644042969\n","SPS: 648\n","SPS: 648\n","SPS: 648\n","global_step=338524, episodic_return=15.59628677368164\n","SPS: 648\n","SPS: 647\n","global_step=339596, episodic_return=141.10140991210938\n","SPS: 647\n","global_step=340016, episodic_return=32.76142501831055\n","SPS: 647\n","SPS: 647\n","global_step=341232, episodic_return=55.80352783203125\n","SPS: 647\n","SPS: 646\n","global_step=342524, episodic_return=37.10652542114258\n","SPS: 645\n","global_step=342920, episodic_return=-115.85404968261719\n","SPS: 645\n","SPS: 644\n","global_step=344016, episodic_return=8.922639846801758\n","SPS: 644\n","SPS: 644\n","SPS: 644\n","global_step=345232, episodic_return=20.625354766845703\n","SPS: 644\n","global_step=346068, episodic_return=-113.03466033935547\n","SPS: 644\n","global_step=346524, episodic_return=27.934843063354492\n","SPS: 644\n","SPS: 644\n","SPS: 644\n","global_step=348016, episodic_return=23.725696563720703\n","SPS: 643\n","SPS: 643\n","SPS: 642\n","global_step=349232, episodic_return=52.74105453491211\n","global_step=349380, episodic_return=-171.18527221679688\n","SPS: 641\n","SPS: 641\n","global_step=350524, episodic_return=8.575212478637695\n","SPS: 641\n","SPS: 640\n","SPS: 640\n","global_step=352016, episodic_return=-57.604087829589844\n","SPS: 640\n","SPS: 640\n","SPS: 640\n","global_step=353380, episodic_return=16.043411254882812\n","SPS: 640\n","global_step=354048, episodic_return=147.962158203125\n","SPS: 640\n","SPS: 640\n","global_step=355144, episodic_return=97.05756378173828\n","SPS: 639\n","global_step=355420, episodic_return=113.4493408203125\n","SPS: 639\n","global_step=356100, episodic_return=-101.42205047607422\n","SPS: 639\n","SPS: 639\n","SPS: 639\n","global_step=357380, episodic_return=68.27192687988281\n","SPS: 639\n","SPS: 639\n","global_step=358840, episodic_return=-145.56533813476562\n","SPS: 638\n","global_step=359144, episodic_return=91.71971130371094\n","SPS: 638\n","SPS: 638\n","global_step=360100, episodic_return=61.3450813293457\n","SPS: 638\n","SPS: 637\n","global_step=361208, episodic_return=170.93814086914062\n","global_step=361380, episodic_return=80.7028579711914\n","SPS: 637\n","SPS: 637\n","SPS: 636\n","SPS: 636\n","global_step=363144, episodic_return=57.052024841308594\n","global_step=363352, episodic_return=-102.84151458740234\n","SPS: 635\n","global_step=363944, episodic_return=101.98875427246094\n","SPS: 635\n","global_step=364320, episodic_return=-141.0006561279297\n","SPS: 635\n","SPS: 636\n","SPS: 635\n","SPS: 635\n","SPS: 634\n","SPS: 633\n","global_step=367144, episodic_return=16.405357360839844\n","global_step=367352, episodic_return=37.21629333496094\n","SPS: 633\n","global_step=367944, episodic_return=58.00123596191406\n","SPS: 633\n","global_step=368320, episodic_return=42.03904342651367\n","SPS: 633\n","SPS: 632\n","SPS: 632\n","global_step=369972, episodic_return=-103.92799377441406\n","SPS: 632\n","SPS: 632\n","global_step=370852, episodic_return=-86.14055633544922\n","global_step=370928, episodic_return=-134.1939239501953\n","SPS: 632\n","global_step=371352, episodic_return=9.42789363861084\n","SPS: 632\n","SPS: 633\n","SPS: 633\n","global_step=372976, episodic_return=-68.43647003173828\n","SPS: 632\n","SPS: 632\n","global_step=373788, episodic_return=102.2789306640625\n","global_step=373972, episodic_return=67.88407897949219\n","global_step=374220, episodic_return=133.83590698242188\n","SPS: 633\n","SPS: 633\n","SPS: 633\n","SPS: 633\n","SPS: 632\n","global_step=376436, episodic_return=191.0123291015625\n","SPS: 631\n","global_step=376976, episodic_return=89.84844207763672\n","SPS: 631\n","SPS: 631\n","global_step=377972, episodic_return=61.41616439819336\n","global_step=378220, episodic_return=42.3503532409668\n","SPS: 631\n","SPS: 631\n","global_step=379188, episodic_return=-39.217552185058594\n","SPS: 631\n","SPS: 631\n","SPS: 631\n","global_step=380436, episodic_return=55.356971740722656\n","SPS: 631\n","global_step=381208, episodic_return=-78.25733184814453\n","SPS: 631\n","SPS: 631\n","global_step=381972, episodic_return=76.28012084960938\n","global_step=382220, episodic_return=54.40501022338867\n","SPS: 631\n","global_step=382644, episodic_return=-104.46837615966797\n","SPS: 631\n","SPS: 631\n","SPS: 630\n","SPS: 630\n","global_step=385024, episodic_return=-96.99854278564453\n","SPS: 630\n","global_step=385208, episodic_return=125.04344177246094\n","SPS: 630\n","global_step=385972, episodic_return=65.38062286376953\n","SPS: 629\n","global_step=386220, episodic_return=92.34205627441406\n","SPS: 630\n","SPS: 630\n","SPS: 630\n","SPS: 629\n","global_step=388468, episodic_return=171.18841552734375\n","SPS: 629\n","global_step=389024, episodic_return=80.3196792602539\n","SPS: 629\n","global_step=389208, episodic_return=54.93810272216797\n","SPS: 629\n","SPS: 629\n","global_step=390220, episodic_return=67.73616790771484\n","SPS: 629\n","global_step=391168, episodic_return=-108.7419662475586\n","SPS: 629\n","global_step=391312, episodic_return=178.0565185546875\n","SPS: 629\n","SPS: 629\n","SPS: 629\n","global_step=392984, episodic_return=-46.90769958496094\n","global_step=393208, episodic_return=55.15465545654297\n","SPS: 628\n","SPS: 628\n","global_step=394220, episodic_return=52.15010070800781\n","SPS: 628\n","SPS: 628\n","global_step=395168, episodic_return=97.0831298828125\n","SPS: 628\n","SPS: 627\n","global_step=395856, episodic_return=-89.89883422851562\n","SPS: 627\n","global_step=396800, episodic_return=-136.49087524414062\n","SPS: 627\n","global_step=396984, episodic_return=12.658827781677246\n","SPS: 627\n","SPS: 627\n","global_step=397864, episodic_return=153.2882080078125\n","SPS: 626\n","SPS: 626\n","global_step=398952, episodic_return=-24.12200164794922\n","SPS: 626\n","global_step=399848, episodic_return=-73.04243469238281\n","global_step=399856, episodic_return=46.66697692871094\n","SPS: 626\n","global_step=399912, episodic_return=192.14627075195312\n","SPS: 626\n","SPS: 627\n","global_step=401360, episodic_return=151.7127685546875\n","SPS: 627\n","global_step=401544, episodic_return=-20.552688598632812\n","SPS: 627\n","SPS: 627\n","global_step=402480, episodic_return=-83.76591491699219\n","SPS: 627\n","SPS: 627\n","global_step=403856, episodic_return=99.95967864990234\n","SPS: 627\n","SPS: 627\n","SPS: 626\n","global_step=405360, episodic_return=20.830801010131836\n","SPS: 625\n","global_step=405544, episodic_return=57.17462158203125\n","global_step=405720, episodic_return=141.53575134277344\n","SPS: 625\n","SPS: 625\n","SPS: 625\n","SPS: 625\n","global_step=407856, episodic_return=55.880794525146484\n","SPS: 624\n","SPS: 624\n","global_step=408744, episodic_return=111.18614196777344\n","SPS: 624\n","global_step=409544, episodic_return=41.35075759887695\n","SPS: 624\n","global_step=409720, episodic_return=91.86331939697266\n","SPS: 624\n","SPS: 624\n","SPS: 624\n","global_step=411172, episodic_return=-145.5491943359375\n","SPS: 624\n","SPS: 623\n","global_step=412596, episodic_return=-164.8953857421875\n","SPS: 623\n","global_step=412744, episodic_return=47.21468734741211\n","SPS: 623\n","global_step=413544, episodic_return=91.358642578125\n","SPS: 624\n","SPS: 624\n","SPS: 624\n","global_step=414916, episodic_return=-34.99451446533203\n","global_step=415172, episodic_return=4.622086048126221\n","SPS: 623\n","SPS: 623\n","SPS: 623\n","global_step=416260, episodic_return=82.16212463378906\n","global_step=416596, episodic_return=69.5950698852539\n","SPS: 623\n","SPS: 624\n","SPS: 623\n","global_step=417824, episodic_return=158.2957763671875\n","global_step=418004, episodic_return=-70.00483703613281\n","SPS: 623\n","SPS: 623\n","global_step=419156, episodic_return=-115.98489379882812\n","SPS: 623\n","SPS: 623\n","global_step=420260, episodic_return=23.042110443115234\n","SPS: 622\n","global_step=420648, episodic_return=-72.29412078857422\n","SPS: 622\n","SPS: 622\n","global_step=421824, episodic_return=51.03123474121094\n","SPS: 622\n","SPS: 622\n","SPS: 622\n","global_step=423156, episodic_return=46.01601028442383\n","SPS: 622\n","SPS: 622\n","global_step=424260, episodic_return=34.54915237426758\n","SPS: 621\n","global_step=424648, episodic_return=65.6918716430664\n","SPS: 622\n","SPS: 622\n","global_step=425824, episodic_return=12.640121459960938\n","SPS: 622\n","SPS: 622\n","SPS: 621\n","global_step=427156, episodic_return=31.07442855834961\n","SPS: 621\n","SPS: 621\n","global_step=428260, episodic_return=75.3933334350586\n","SPS: 621\n","global_step=428648, episodic_return=44.4675178527832\n","SPS: 621\n","SPS: 621\n","global_step=429824, episodic_return=55.656898498535156\n","SPS: 621\n","SPS: 621\n","SPS: 620\n","global_step=431148, episodic_return=146.47952270507812\n","global_step=431156, episodic_return=99.17028045654297\n","SPS: 620\n","SPS: 620\n","global_step=432260, episodic_return=54.24041748046875\n","global_step=432404, episodic_return=-74.96066284179688\n","SPS: 621\n","SPS: 621\n","global_step=433548, episodic_return=-104.9388427734375\n","SPS: 620\n","SPS: 620\n","SPS: 619\n","global_step=435148, episodic_return=30.334857940673828\n","SPS: 619\n","SPS: 619\n","SPS: 619\n","global_step=436260, episodic_return=92.50579833984375\n","global_step=436404, episodic_return=74.24533081054688\n","SPS: 619\n","SPS: 619\n","global_step=437548, episodic_return=70.2569351196289\n","SPS: 619\n","SPS: 619\n","global_step=438708, episodic_return=201.68069458007812\n","SPS: 618\n","global_step=439148, episodic_return=51.85919952392578\n","SPS: 618\n","SPS: 618\n","global_step=440260, episodic_return=48.61940383911133\n","SPS: 618\n","SPS: 618\n","SPS: 617\n","global_step=441484, episodic_return=-127.4554443359375\n","global_step=441548, episodic_return=26.553314208984375\n","SPS: 617\n","SPS: 617\n","global_step=442712, episodic_return=-83.5584716796875\n","SPS: 617\n","global_step=443148, episodic_return=77.13807678222656\n","SPS: 617\n","global_step=443708, episodic_return=-41.63368606567383\n","SPS: 618\n","global_step=443968, episodic_return=-103.07994079589844\n","SPS: 618\n","SPS: 618\n","SPS: 618\n","global_step=445844, episodic_return=-78.39115905761719\n","global_step=445944, episodic_return=156.65650939941406\n","SPS: 618\n","SPS: 618\n","global_step=446712, episodic_return=50.877803802490234\n","SPS: 618\n","SPS: 618\n","global_step=447968, episodic_return=64.69100189208984\n","SPS: 617\n","global_step=448456, episodic_return=-129.25526428222656\n","SPS: 617\n","SPS: 617\n","SPS: 617\n","global_step=449844, episodic_return=72.84298706054688\n","global_step=450036, episodic_return=140.8233184814453\n","SPS: 617\n","SPS: 617\n","SPS: 617\n","global_step=451436, episodic_return=114.32249450683594\n","SPS: 617\n","SPS: 617\n","global_step=452124, episodic_return=151.41476440429688\n","global_step=452456, episodic_return=35.30519485473633\n","SPS: 617\n","SPS: 617\n","SPS: 617\n","global_step=454036, episodic_return=130.76516723632812\n","SPS: 617\n","SPS: 617\n","SPS: 616\n","global_step=455436, episodic_return=35.509971618652344\n","global_step=455648, episodic_return=135.74066162109375\n","SPS: 616\n","SPS: 616\n","global_step=456456, episodic_return=53.623939514160156\n","SPS: 616\n","SPS: 616\n","SPS: 616\n","global_step=458036, episodic_return=69.8526611328125\n","SPS: 616\n","SPS: 616\n","global_step=458768, episodic_return=135.5750274658203\n","global_step=458928, episodic_return=-105.49832916259766\n","SPS: 616\n","global_step=459436, episodic_return=25.55476188659668\n","SPS: 616\n","SPS: 616\n","global_step=460772, episodic_return=125.32276153564453\n","SPS: 616\n","SPS: 616\n","global_step=461456, episodic_return=146.1143798828125\n","SPS: 616\n","global_step=462128, episodic_return=-102.05077362060547\n","SPS: 616\n","global_step=462452, episodic_return=119.74312591552734\n","SPS: 616\n","global_step=463288, episodic_return=-65.48140716552734\n","SPS: 616\n","SPS: 615\n","global_step=464032, episodic_return=-68.25418090820312\n","SPS: 615\n","SPS: 615\n","SPS: 615\n","global_step=465456, episodic_return=65.68669891357422\n","SPS: 615\n","SPS: 615\n","global_step=466452, episodic_return=42.05687713623047\n","global_step=466588, episodic_return=100.19283294677734\n","SPS: 615\n","SPS: 615\n","SPS: 615\n","global_step=468032, episodic_return=72.29706573486328\n","SPS: 615\n","global_step=468636, episodic_return=124.665771484375\n","SPS: 615\n","global_step=469000, episodic_return=-92.04125213623047\n","SPS: 615\n","SPS: 615\n","SPS: 614\n","global_step=470588, episodic_return=79.90890502929688\n","SPS: 614\n","SPS: 614\n","global_step=472032, episodic_return=82.57508087158203\n","SPS: 614\n","SPS: 614\n","global_step=472636, episodic_return=68.4369125366211\n","global_step=473000, episodic_return=46.58638381958008\n","SPS: 613\n","SPS: 614\n","SPS: 614\n","global_step=474588, episodic_return=46.7564811706543\n","SPS: 613\n","SPS: 613\n","global_step=475392, episodic_return=-62.976016998291016\n","SPS: 613\n","global_step=476032, episodic_return=53.637115478515625\n","SPS: 614\n","SPS: 614\n","global_step=477000, episodic_return=110.07243347167969\n","global_step=477168, episodic_return=-70.78744506835938\n","SPS: 613\n","SPS: 613\n","SPS: 613\n","SPS: 613\n","SPS: 612\n","global_step=479392, episodic_return=84.19613647460938\n","SPS: 612\n","global_step=479852, episodic_return=129.55960083007812\n","global_step=480032, episodic_return=83.92951965332031\n","SPS: 612\n","SPS: 612\n","global_step=481168, episodic_return=47.82914352416992\n","SPS: 612\n","SPS: 612\n","global_step=482116, episodic_return=117.11405181884766\n","SPS: 612\n","SPS: 612\n","global_step=482852, episodic_return=171.80038452148438\n","SPS: 611\n","SPS: 611\n","global_step=483852, episodic_return=74.69176483154297\n","SPS: 611\n","SPS: 611\n","global_step=485168, episodic_return=33.56224060058594\n","SPS: 611\n","SPS: 611\n","global_step=486116, episodic_return=34.29176712036133\n","SPS: 611\n","global_step=486852, episodic_return=64.90664672851562\n","SPS: 611\n","global_step=486972, episodic_return=-43.36583709716797\n","SPS: 611\n","global_step=487456, episodic_return=106.75013732910156\n","SPS: 611\n","SPS: 611\n","SPS: 611\n","SPS: 611\n","global_step=489628, episodic_return=154.48362731933594\n","global_step=489804, episodic_return=-93.58942413330078\n","SPS: 611\n","global_step=490116, episodic_return=70.5306167602539\n","global_step=490304, episodic_return=139.41513061523438\n","SPS: 611\n","SPS: 611\n","SPS: 611\n","SPS: 611\n","SPS: 610\n","global_step=492816, episodic_return=149.04302978515625\n","global_step=493020, episodic_return=160.8064422607422\n","SPS: 610\n","SPS: 610\n","global_step=493628, episodic_return=38.12090301513672\n","global_step=493804, episodic_return=32.093177795410156\n","SPS: 610\n","SPS: 610\n","SPS: 610\n","global_step=495228, episodic_return=-61.5328254699707\n","SPS: 610\n","SPS: 610\n","SPS: 610\n","global_step=496664, episodic_return=144.84201049804688\n","global_step=496696, episodic_return=103.89044952392578\n","global_step=497020, episodic_return=66.14915466308594\n","SPS: 610\n","SPS: 610\n","SPS: 610\n","SPS: 609\n","SPS: 609\n","global_step=499228, episodic_return=55.245182037353516\n","SPS: 609\n","\u001b[38;5;4m‚Ñπ This function will save, evaluate, generate a video of your agent,\n","create a model card and push everything to the hub. It might take up to 1min.\n","This is a work in progress: if you encounter a bug, please open an issue.\u001b[0m\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n","\u001b[1;34m[swscaler @ 0x6210440] \u001b[0m\u001b[0;33mWarning: data is not aligned! This can lead to a speed loss\n","\u001b[0m\u001b[38;5;4m‚Ñπ Pushing repo BanUrsus/my-ppo-LunarLander-v2 to the Hugging Face\n","Hub\u001b[0m\n","Upload 2 LFS files:   0% 0/2 [00:00<?, ?it/s]\n","model.pt:   0% 0.00/42.6k [00:00<?, ?B/s]\u001b[A\n","\n","events.out.tfevents.1695096220.37e980d23d4f.9777.0:   0% 0.00/654k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","events.out.tfevents.1695096220.37e980d23d4f.9777.0:   3% 16.4k/654k [00:00<00:09, 66.7kB/s]\u001b[A\u001b[A\n","model.pt:  38% 16.4k/42.6k [00:00<00:00, 63.6kB/s]\u001b[A\n","\n","model.pt: 100% 42.6k/42.6k [00:00<00:00, 71.5kB/s]\n","events.out.tfevents.1695096220.37e980d23d4f.9777.0: 100% 654k/654k [00:00<00:00, 956kB/s] \n","Upload 2 LFS files: 100% 2/2 [00:00<00:00,  2.14it/s]\n","\u001b[38;5;4m‚Ñπ Your model is pushed to the Hub. You can view your model here:\n","https://huggingface.co/BanUrsus/my-ppo-LunarLander-v2/tree/main/\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"eVsVJ5AdqLE7"},"source":["## Some additional challenges üèÜ\n","The best way to learn **is to try things by your own**! Why not trying  another environment?\n"]},{"cell_type":"markdown","metadata":{"id":"nYdl758GqLXT"},"source":["See you on Unit 8, part 2 where we going to train agents to play Doom üî•\n","## Keep learning, stay awesome ü§ó"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part1.ipynb","timestamp":1694741767462}],"gpuType":"T4","collapsed_sections":["xWwRLDoxaIqA","mk-a9CmNuS2W"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"52c7dd910ca640f78825e19534953c27":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_72f563c9e5864344b42b6a3c75d51628","IPY_MODEL_54ec2df88de2410c83bfbd36dfe02af8","IPY_MODEL_141b3eb578b842c8971ea9fc34b8c63a","IPY_MODEL_dbcf2e3ef5dd4bd99c5dc23e9d20ee8e"],"layout":"IPY_MODEL_f5b80e76e63645e59a909c0b1bd9dc1d"}},"6d52daec44c340eeb49b58a8da65dca7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b96c87489a44700a41b94fa4fd94386","placeholder":"‚Äã","style":"IPY_MODEL_e18f717d509f4a90b61f1703274684c7","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"c9ec0f72f8b44456a7f86c2035cc107c":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_f48df99ebe894267b91e981f72733195","placeholder":"‚Äã","style":"IPY_MODEL_45e51deaff344f5cab945e05c9bf4f16","value":""}},"e9094548b1fe448cbbda858c8b7565f5":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_9f2d2e1f311448fb9f9cc81fdbc2de3a","style":"IPY_MODEL_655571ac35eb4f44a1274d2def38e9f1","value":true}},"d33d91c430504cf49be1bfab95b08279":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_aba645507b654bd9ad7020163bf6f032","style":"IPY_MODEL_4399f266eb5441c29e8c7c3dd113e52e","tooltip":""}},"2e5376eb4ab64230a2a5fbe8ba806ff1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_848b344d72254a3c9dc5072f13f417b5","placeholder":"‚Äã","style":"IPY_MODEL_0013a08a3d0246a68ab7102157331918","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"f5b80e76e63645e59a909c0b1bd9dc1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"8b96c87489a44700a41b94fa4fd94386":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e18f717d509f4a90b61f1703274684c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f48df99ebe894267b91e981f72733195":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45e51deaff344f5cab945e05c9bf4f16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f2d2e1f311448fb9f9cc81fdbc2de3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"655571ac35eb4f44a1274d2def38e9f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aba645507b654bd9ad7020163bf6f032":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4399f266eb5441c29e8c7c3dd113e52e":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"848b344d72254a3c9dc5072f13f417b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0013a08a3d0246a68ab7102157331918":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"579e5d9d0dbd4d8abfaf0a3c0acc56e4":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_828c6f75928b47708578f9f419848651","placeholder":"‚Äã","style":"IPY_MODEL_c112f2d4b3494caf85b17571dc94fc8f","value":"Connecting..."}},"828c6f75928b47708578f9f419848651":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c112f2d4b3494caf85b17571dc94fc8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72f563c9e5864344b42b6a3c75d51628":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44203070f32a4bc6a7cfc386f601ea70","placeholder":"‚Äã","style":"IPY_MODEL_16e1eff35c554617b339b1bdd0689973","value":"Token is valid (permission: write)."}},"54ec2df88de2410c83bfbd36dfe02af8":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d9d97f9878a436483d242eb8f90abf8","placeholder":"‚Äã","style":"IPY_MODEL_ebc603fcd0244850ac6ca40ac0a16f12","value":"Your token has been saved in your configured git credential helpers (store)."}},"141b3eb578b842c8971ea9fc34b8c63a":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71007af36dae4de5a06e7ce404478014","placeholder":"‚Äã","style":"IPY_MODEL_f6b1354b01874e15bd66c56c45b85d9e","value":"Your token has been saved to /root/.cache/huggingface/token"}},"dbcf2e3ef5dd4bd99c5dc23e9d20ee8e":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b563dd0886f34242bb3b45b3daebc445","placeholder":"‚Äã","style":"IPY_MODEL_144c6ed281e040d09f0675d8536ada3a","value":"Login successful"}},"44203070f32a4bc6a7cfc386f601ea70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16e1eff35c554617b339b1bdd0689973":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d9d97f9878a436483d242eb8f90abf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebc603fcd0244850ac6ca40ac0a16f12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71007af36dae4de5a06e7ce404478014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6b1354b01874e15bd66c56c45b85d9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b563dd0886f34242bb3b45b3daebc445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"144c6ed281e040d09f0675d8536ada3a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}